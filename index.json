[
{
	"uri": "https://kondor6c.github.io/posts/golang_with_no_googling/",
	"title": "Golang with no Googling",
	"tags": ["golang", "learning", "linux"],
	"description": "As an exercise, I tried writing Go without Googling",
	"content": "I spent a good amount of time learning Golang (Go, but since using two letters in any form of a search usually is an exercise in futility). My goal was fairly simple, I wanted to display some system statistics via HTTP by reading them directly from /proc. I saw that Golang had a module in the standard library call \u0026quot;syscall\u0026quot; and I decided that it might be fun to give that a try. Since I have seen a fair amount of syscalls by using strace and could always use trusty man pages, I thought this might be interesting and I might learn something in the process. I also was feeling fairly confident with the langauge after ingesting data from the web with Bleve (Golang Elasticsearch). It seems many of my results have been from Medium or StackOverflow, I really don't care for Medium, I get the feeling that it is tracking me; additionally the annoying notification that I need to sign up reminds me of Quora. I knew from experience that generally I would need to stat the file, then read and write to the location I wanted. A fairly easy operation, having considered all things that could be involved. I could probably even skip the stat since I know it is there and that I can open it. To check my thought process and keeping with my no internet searching exercise, I did an strace on cat just to check my thought process.\nexecve(\u0026quot;/bin/cat\u0026quot;, [\u0026quot;cat\u0026quot;, \u0026quot;/proc/interrupts\u0026quot;], 0x7ffd84c42188 /* 82 vars */) = 0 brk(NULL) = 0x560adc986000 access(\u0026quot;/etc/ld.so.preload\u0026quot;, R_OK) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 fstat(3LL, 1886056, PROT_READ, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f6688407000 mprotect(0x7f6688429000, 1708032, PROT_NONE) = 0 mmap(0x7f6688429000, 1404928, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x22000) = 0x7f6688429000 mmap(0x7f6688580000, 299008, PROT_READ, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x179000) = 0x7f6688580000 mmap(0x7f66885ca000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1c2000) = 0x7f66885ca000 mmap(0x7f66885d0000, 14184, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f66885d0000 close(3) = 0 arch_prctl(ARCH_SET_FS, 0x7f66885d5540) = 0 mprotect(0x7f66885ca000, 16384, PROT_READ) = 0 mprotect(0x560adba2a000, 4096, PROT_READ) = 0 mprotect(0x7f6688637000, 4096, PROT_READ) = 0 munmap(0x7f66885d6000, 236364) = 0 brk(NULL) = 0x560adc986000 brk(0x560adc9a7000) = 0x560adc9a7000 openat(AT_FDCWD, \u0026quot;/usr/lib64/locale/locale-archive\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 fstat(3, {st_mode=S_IFREG|0644, st_size=4554608, ...}) = 0 mmap(NULL, 4554608, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f6687faf000 close(3) = 0 fstat(1, {st_mode=S_IFCHR|0600, st_rdev=makedev(0x88, 0x2), ...}) = 0 openat(AT_FDCWD, \u0026quot;/proc/interrupts\u0026quot;, O_RDONLY) = 3 fstat(3, {st_mode=S_IFREG|0444, st_size=0, ...}) = 0 fadvise64(3, 0, 0, POSIX_FADV_SEQUENTIAL) = 0  Pretty trivial, as I thought, but I looked at the AT_FDCWD. This is due to the fact the file being opened might be relative. I then tried to use Golang to grab the current working directory. Using Godoc this was my simple result.\npackage main // an exercise where I am not going to \u0026quot;google\u0026quot; anything, use only reference docs  import ( \u0026quot;fmt\u0026quot; \u0026quot;syscall\u0026quot; ) func main() { var cwd_fd int var err error cwd_fd, err := syscall.Getcwd([]byte(\u0026quot;/\u0026quot;)) if err != nil { panic(err) } fmt.Println(cwd_fd) }  Upon running this I got a panic, err was hit and it said that\npanic: numerical result out of range  I double checked Godoc and it does return and int, I tried an int64 and that was indeed the wrong type, I also tried to automatically assign the type using good ole \u0026quot;:=\u0026quot; and got the same result. I still plan on continuing this exercise, maybe even have this little project/program be specifically dedicated to not searching. Like a search free clean room implementation. I still don't know if others have encountered this, but the syscall page doesn't have much for documentation (probably because many of these are already standard and documented elsewhere). The overview of the docs does say that this module is deprecated, but I wonder why it won't work at all, I also wonder if the GCC Go might have a different result. Another fun excercise might be to compare the benchmarks of both to see which might perform better. I would think that the syscall might since there isn't any guessing or fringe attempts that would be going on. This attempt was neat to do and I actually recommend it! Have fun.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/gnutls_ed25519/",
	"title": "GnuTLS and ed25519",
	"tags": ["linux", "certs", "gnutls"],
	"description": "GNUTLS 3.6 brings ed25519 key support",
	"content": "I\u0026rsquo;m very excited that GNUTLS has support for ed25519 keys, these keys are faster, smaller, and more secure than RSA keys. I found this out from a developer\u0026rsquo;s blog while doing research and messing around with CA\u0026rsquo;s in December. The reason I\u0026rsquo;m writing this is that GNUTLS 3.6 has been made available in Fedora 28, therefore can all use this. Additionally and perhaps more importantly we gain the the ability to use TLS 1.3, which cloudflare has [written][1] [about][2]. The arrival of this support is slightly bitter sweet because we do lose support for GPG based x509 [certificates][3]. This is unfortunate in principle because the CA\u0026rsquo;s that are apporved can impose a signifigant cost to an individual, I certainly have not used GPG based x509 certificates. I don\u0026rsquo;t believe this is due an issue with lack of interest, I believe it is due to the difficulty of GPG and the fact that the end user often needs to have some kind of knowledge of PKI and how GPG does it; which I have found to be lacking. But like the GNUTLS developer [wrote][4], we can use Let\u0026rsquo;s Encrypt, but it is hard to say what might be in Let\u0026rsquo;s Encrypt\u0026rsquo;s future. I sincerely wish it the best, it is nice to have competing or overlapping technologies, having the extra functionality does pose a couple issues because it increases an attack vector and more maintenance would be required on the software. I haven\u0026rsquo;t found many people to use GNUTLS, but maybe people might with Kubnetes (additionally there have been concerns with OpenSSL). I guess we have that and we can roll our own CA! More on that later!\nWe should briefly look at creating an ed25519 key with GNUTLS:\ncerttool --generate-privkey --key-type ed25519 --outfile key-ed25519.pem  That\u0026rsquo;s it! Really simple, and personally I think it is much easier and readable than OpenSSL.\n[1] https://blog.cloudflare.com/why-iot-is-insecure/ [2] https://blog.cloudflare.com/you-get-tls-1-3-you-get-tls-1-3-everyone-gets-tls-1-3/ [3] https://www.gnutls.org/manual/html_node/OpenPGP-certificates.html [4] https://nikmav.blogspot.com/2017/\n"
},
{
	"uri": "https://kondor6c.github.io/posts/end_of_disk_issues/",
	"title": "An End of Disk Issues?",
	"tags": ["linux", "zfs", "hardware"],
	"description": "A hopeful end to disk issues on my workstation and ending with a brief mention of topics to come",
	"content": "I have a workstation that I have built, it contains a multi-disk Large Form Factor ZFS array. There have been multiple posts in the past about some of my disk related problems that I\u0026rsquo;ve handled, you should be able to find them with a \u0026lsquo;zfs\u0026rsquo; or \u0026lsquo;disk\u0026rsquo; tag. Well I had several issues, that I hope should be calming down; going back to January, where I decided to move from 4x3TB RAID-Z (software RAID 5) to 4x4TB 7200RPM RAID 10 setup. The transition was prompted by this event and I really have written this up to share my experience with others. I spoke casually about this with Jim Salter, Michael Hrivnak, Wes Widner, and several others, but I brought it up mostly because of Jim\u0026rsquo;s project Sanoid and he\u0026rsquo;s presented on ZFS before. I had one disk start clicking when I got back from running, I could see in dmesg that this disk was not responding and was trying to be reset; important thing to point out, ZFS had brought the disk OFFLINE. I had a spare 4TB \u0026lsquo;eco\u0026rsquo; drive that really doesn\u0026rsquo;t deliver the performance that is needed and would hinder the array, but I wanted to get the disk replaced so that I could get another one ordered. Well I hooked the disk up, and started the zfs replace process. I woke up and found that another disk, in this RAID-Z, had reported as failing (FAULTED), all during the replace process. I knew this was very bad, two disks failing, and you can\u0026rsquo;t stop a resilver, but you can stop a scrub, by using zpool scrub -s ${pool}, what I should have done is brought the disk \u0026ldquo;online\u0026rdquo; but what I did, could have been worse\u0026hellip; I rebooted my workstation! During the resilver/replace process, not my finest hour. This array doesn\u0026rsquo;t hold too much, but it does hold \u0026ldquo;warm\u0026rdquo; backups (staged to go on to different disk stored elsewhere), KVM disk images (virtual machines), and other various data, usually high write IO that I don\u0026rsquo;t want to wear on my SSD/NVMe drive, I could stand to lose this array, but I don\u0026rsquo;t want to because it would be painful. When the machine came back, the pool was imported and the resilver restarted, this time with the clicking disk \u0026ldquo;ONLINE\u0026rdquo; however the other faulted disk, was now \u0026ldquo;UNAVAILABLE\u0026rdquo;, so I have a replace going of an online disk that is in a poor state, but ZFS had recovered from this, and the replace completed successfully. I then transferred the data elsewhere, rebuilt the pool as a RAID-10. The pool is now with all 7200 RPM disks in a RAID-10 setup. ZFS looks at this as one mirror (RAID 1) with a striped disk set (RAID 0), this might be confusing, I will perhaps try to do a video to cover this more. Here is some output of my now healthy disk status:\n NAME STATE READ WRITE CKSUM extra ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 wwn-0x5000cca23dcdcfc7 ONLINE 0 0 0 wwn-0x5000c500675738ba ONLINE 0 0 0 mirror-1 ONLINE 0 0 0 wwn-0x5000cca22bc39ed8 ONLINE 0 0 0 wwn-0x50000397fc580978 ONLINE 0 0 0  The issue that I think I had encountered for a while, was that I plugged three 4TB 7200RPM disks using the same SATA power plug coming from the power supply. I had thought I was up against a bad controller/port/cable and I plugged a red SATA cable in to signify that \u0026ldquo;red is dead\u0026rdquo;, but the issue continued when I plugged it into another SATA port. I realized that when I put the hard drive in a top unit or in a USB3 SATA drive, that the disk was fine it didn\u0026rsquo;t even spin cycle. I believe that this new disk array was consuming more power enough that the disk would power on, but upon doing load wouldn\u0026rsquo;t service the ATA commands quick enough, and linux would then reset it. Similar to this\n[Wed Jun 6 12:51:54 2018] ata1: link is slow to respond, please be patient (ready=0) [Wed Jun 6 12:52:23 2018] ata1: COMRESET failed (errno=-16) [Wed Jun 6 12:52:23 2018] ata1: hard resetting link [Wed Jun 6 12:52:28 2018] ata1: COMRESET failed (errno=-16) [Wed Jun 6 12:52:28 2018] ata1: reset failed, giving up  Giving this \u0026ldquo;bad disk\u0026rdquo; a dedicated power plug resolved these issues. I had a similar issue when adding five 1U HP DL160\u0026rsquo;s to a Cabinet, Friday night I hooked them all up, and we started the HDFS rebalance, but upon getting load on those machines, we got a phone call from Sungard saying that we\u0026rsquo;re exceeding the power limit. Load can influence things, in many ways, I knew this; I guess this one just took me longer to realize.\nIn other news, I have created a video on my camera showing the process of using a Yubikey and pass to get versioned passwords, but I realized that it was very ad-hoc and I deleted the video. I spent a fair amount of time editing in KDEnlive and audacity, but I just was not pleased with the end product so I will be redoing it and uploading it shortly. I also plan on writing up my home router creation and performance testing of MySQL as I hinted at last blog post. I did some performance testing of docker storage drivers and needless to say we will be avoiding btrfs, I didn\u0026rsquo;t test with nodatacow as a mount option, but really XFS and overlay2 didn\u0026rsquo;t have that option and performed very well. This test wasn\u0026rsquo;t exhaustive, but enough to give an idea and fulfill proper investigation/baselining to change the storage drivers that my employer is using, without getting into too many details, a contractor configured us to use one that should not be used in production.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/self-2018/",
	"title": "SELF 2018",
	"tags": ["community", "blogging", "conferences", "real world"],
	"description": "Reflection of the SouthEast LinuxFest 2018",
	"content": "I really enjoyed SouthEast LinuxFest this year, probably more this year than I did last year. Mostly because I made a point of being more outgoing and interacting with more people. Last year I attended I had tried talking to a few people but didn\u0026rsquo;t really have a good feel for things, and volunteered for the FSF booth and tried to take the Ham radio exam. This year I attended many presentations, had an opportunity to go to a Birds Of a Feather meeting on containers put on by Michael at RedHat. Additionally I met and at least briefly talked to Eric Raymond, Noah from the Ask Noah show, and some of the gang from the Linux Link Tech Show. If I met and hung out with you, thank you I really did have a great time and I hope we can continue to stay in contact. It really is hard to stay in contact especially when people are very far away. However it can be very rewarding to meet back with the people at a conference or festival or getting something to eat or drink. Additionally when you even talk about technology both parties often walk with something. I certainly enjoy sharing even giving advise, a secret config setting, or a new feature.\nOn my journey to SELF; on Friday I had to pick up my daughter from day care so I knew I wouldn\u0026rsquo;t be able to make it. On my way down Saturday morning I got a speeding ticket, some guy in a black BMW was tailgating me and I had tried to speed up and get out of the way, then he merged behind me and continued to do so, then I tried to speed up and get around a van and he still followed behind me. I explained this to the state trooper, I suspect they had extra enforcement because of this a wreck and complaints that there is rampant speeding (I agree there typically is a lot of speeding). I don\u0026rsquo;t speed much any more, consider I was driving in an older prius after I surrendered my Volkswagen Jetta Sportwagen TDI (due to the Federal emissions issue).\nOn my way back home I ended up getting caught in the traffic from this fatal incident, I had to go all across Lake Norman, I got home at 3, and fell asleep around 3:45. Sunday was fairly easy I will try and post links to some of the talks I attended, I really enjoyed Percona\u0026rsquo;s troubleshooting and analyzing database performance with PMM; that was done by the Percona CEO who spoke with me afterwards. I was really happy about all of that, and I might try and get some benchmarks of my own, I do like analyzing performance on many different level going back to other blog articles like using perf. However they aren\u0026rsquo;t to the caliber I would like them to be at. I\u0026rsquo;m just trying to produce content so that I can get into a pattern/habit of writing more.\nOn my commutes I thought that I would really like to try and present, I had done some presentations to the Phoenix Linux Users Group. But when I went a deployment with the Army I lost some momentum. I am committing to doing a speech next year given that my speech/paper is accepted, but I haven\u0026rsquo;t determined what my topic will be I could cover toolsets that I use at work, but really that is what https://stackshare.io/ is for I think, I don\u0026rsquo;t know if many people will get a lot out of that. So I have thought about these topics: - Quagga/Bird Routing software for Linux - Kernel tunable via sysctl - iptables GPU offloading - fuzzing an open source project to assist with security analysis\nOr focus on some directed core content, like SSL certificates, or the Percona CEO giving a speech about analyzing performance, here are some ideas: - SSL certificates, again I still think many don\u0026rsquo;t know this area as well as they should - git porcelain and plumbing - Linux powertools like nmap, qutebrowser, copyq, autokey/xbindkeys, xidotools, xautomate Some of these could be prime for an easier ones targeted for a Sunday afternoon when people are about ready to leave, and they also can be done by me independently and I can make the content available through other means such as vimeo/youtube/archive.org I don\u0026rsquo;t entirely know, but I have time to polish something up. I also hope that by committing here to doing this it will help to re-enforce that. Sorry if this is either long or scattered, I really am very tired, and I might go back and edit this later.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/resume-blogging/",
	"title": "Resume Blogging",
	"tags": ["blogging", "real world", "non-tech"],
	"description": "After a number of continuing situations, I resume blogging",
	"content": " After a few issues that almost perfectly lined up to steal momentum I\u0026rsquo;m going to resume blogging. If you follow this (I doubt many were/are), I apologize for the lack of updates. I\u0026rsquo;ll try to briefly run down some of the events that lead to the large (almost year delay):\nThe Prior Events  Before SouthEast Linux Fest my Wife\u0026rsquo;s mother very unexpectedly died. After SELF I had some motivation and tried to look into interesting topics, generally Software Defined Radio (I liked the LimeSDR) and followed some crypto currency investigation which lead to Zcash. Shortly after I was looking into creating an RPM for Zcash, we had a pipe burst in our house around the kitchen sink and that fell directly on my workstation. This one situation alone was perhaps the biggest issue, I didn\u0026rsquo;t have my personal workstation and that severly hampered my projects and work. I ended up buying a couple motherboards off ebay and NewEgg which were all dead on arrival, all Asus Z10-D16 WS (LGA 2011-v3 Haswell based Xeon\u0026rsquo;s) had BIOS issues, I had RMA\u0026rsquo;d a Z10-D16 PE that failed a BIOS update, this whole process ended up causing a 3 month disruption to me and my family. October 20th, my wife texted me saying her water broke and our first daughter was born on the 21st. Subsequently, I missed All Things Open 2017, which I was really looking forward to attending since Jeff Altwood was doing the keynote. I prepaid and everything haha. In December 2017, work/employer had major issues with Amazon\u0026rsquo;s Aurora and what appeared to be (using NewRelic) long connection times, I had painstakingly tracked these down to be delay in resolving the DNS entries (querying Route53), some of them ended up having a 3 second delay, and it happened fairly frequently. This timeframe was our busy season and my wife went back to work and we got a few lessons in parenting. I got the flu on New Years Eve! This really sucked, and was not like anything I had before, even my first dose of the Anthrax shot sucked, but this was way worse and other flu\u0026rsquo;s were not nearly as bad as this year (2018). Then almost 3 days after I went to work, my wife brought home the norovirus from work (she works as a nurse at a local hospital). Dual disk failure event on my ZFS array, I\u0026rsquo;ll document this later I am still very pleased with ZFS on Linux. I got a new job working at Early Warning as a Senior Devops Engineer, things were hot and heavy when I got there, but now things have calmed down some and I\u0026rsquo;m learning more about the environment. One of these alone (maybe not the pipe breakage) might have been easier to brush off. But these events seemed to happen right after each other, and just really seemed to keep me from picking things back up.  Going Forward, Future Plans So I have thought about trying to resume, sharing information and source code, not only that but the community those are some of the tenants that comprise my focus. That I\u0026rsquo;m going to try and start fresh, using a static content blog and move to a more planned deliberate site. I moved the content to Pelican, most of the content I create is static and I don\u0026rsquo;t have a real need for something that is more dynamic. Both store their files as plain text which make disaster recovery much easier. Here are a few principles that I will be focusing on: - The primary focus should/will be the blog and its content - There will be (nor have there ever been) any schedules, I feel that they just lead to missed deadlines and excuses, and on my side it just creates frustration and contention. - I will be focusing on tagging content better and remaining consistent with those tags, even trying to go back through the older content. - There will be no \u0026ldquo;sponsorships\u0026rdquo; or advertisements. - Creating a couple different categories/partitions, one will be for configurations or one line scripts that I frequently access, this is more for content reference. The other will be Hardware, various hardware components that I have bought, used or otherwise might be important to share. I hope that it might help people either not make the same mistake or get a good value from something. - I would like to create more videos, and make them better, there was a comment about loud typing sounds in the one and first video I created on SHA1 OpenSSL certificate chains, loud typing sounds. I agree, although I removed the comment, it still resonates with me and I hope I can improve. - I would like for suggestions to be made on content, I don\u0026rsquo;t know how this will work, but maybe with github/gitlab/pagure.io issues, and then having votes on those topics. I think that might come easier when I have earned a reputation of delivering and the quality of that product.\nVery soon I hope to create a few issues, ranging from my most recent expedition in to creating a home router with Gentoo, and recording the ongoing ZFS usage, the one liners I use, and hardware/things I interact with. I genuinely hope that I can help others.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/openssl-video/",
	"title": "OpenSSL Video",
	"tags": ["openssl", "certs", "video", "blogging"],
	"description": "",
	"content": "New video I've wanted to make videos for a while now, and I finally have. I rehearsed for a couple iterations now, I wrote the outline and all the commands out before hand; [[public:linux:common#openssl|here is where you can view the outline/commands]] . Everything went pretty smoothly, everybody I spoke to in my circle spoke positively about it, I wish I could get a couple more reactions about the technical content. I tried to upload the video to a public locations: vimeo \u0026lt;https://vimeo.com/196973859\u0026gt;, youtube \u0026lt;https://www.youtube.com/watch?v=KXi3-3dEb8k\u0026gt;, and perhaps more importantly archive.org \u0026lt;https://archive.org/details/GeneratingSslCertificatesInLinux\u0026gt; . Although it might not get that many views there I would like the content to be publicly accessible and really without advertisements. Now you can grab the torrent \u0026lt;https://archive.org/download/GeneratingSslCertificatesInLinux/GeneratingSslCertificatesInLinux_archive.torrent\u0026gt; and share it so that others can view it freely.\nI plan on making other videos, if you have ideas or feedback on past videos email me (kondor6c\u0026#64;lazytree.us). Here are some of my ideas on the next video courses: Editors (vim/emacs/nano) Web servers and dokuwiki (Apache, Nginx) debugging using Packet capturing (focus on SSL and errors) operational topics (like df, top, ps, free, du, kill, rm, find) reverse proxy and a Java web application server automating previous topics    I hope that this will help other people. If you have ideas or again feedback, let me know. I used qt-recordmydesktop and edited it with openshot, the microphone worked out well, which is a Blue (company) Yeti (model). I would encourage you to accelerate the speed of the recording, I spoke with my wife and we both agree the recording sounds more professional/better at 1.5x speed.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/urxvt_vs_eterm/",
	"title": "urxvt_vs_eterm",
	"tags": ["linux"],
	"description": "Brief look at performance with perf and a couple terminals",
	"content": "====== urxvt vs Eterm ====== So I was trying different terminals so that I can keep on terminal just for a screencast sessions. I want to keep screencasting clean and partitioned. Additionally I have admitted to myself (now everyone else) that I need to know more about profiling the system, I got http://www.brendangregg.com/books.html on system performance, so far its very through. One complaint is that as a Solaris guy and even regarding many of his Linux presentations and writings is that he really favors dtrace, for obvious reasons; its a fantastic tracing tool. However due to license restrictions it isn\u0026rsquo;t available on Linux (except for http://www.oracle.com/technetwork/articles/servers-storage-dev/dtrace-on-linux-1956556.html). So I have first been reading up and trying out tools like perf. All the while I kept reading that urxvt has a lower footprint than many other terminals; in order to get some of these metrics they were using metrics https://www.void.gr/kargig/blog/2008/06/22/the-quest-for-a-better-rxvt-unicode-on-gentoo/ https://forums.gentoo.org/viewtopic-t-539368-view-previous.html?sid=d495c38e4482f4e9e59e1f969fc45f2e and https://bbs.archlinux.org/viewtopic.php?id=125217 output\u0026hellip;. While other use https://bbs.archlinux.org/viewtopic.php?id=65634 and http://www.calno.com/evilvte/\u0026hellip;. tisk tisk. Where has the massive use of htop come from? I understand it has colors, but it doesn\u0026rsquo;t bring much more to the table (and you have to install it)! Well I remembered using Eterm, and that it had http://arstechnica.com/features/2000/03/simd/, I thought that I would give perf a try to figure this out. This output is far from through, and is extremely basic (I would like to do some more analyzing)\n[kondor6c@horse ~]$ perf stat -d Eterm Performance counter stats for 'Eterm': 149.269413 task-clock:u (msec) # 0.064 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 6,141 page-faults:u # 0.041 M/sec 170,152,520 cycles:u # 1.140 GHz (54.58%) 333,404,464 instructions:u # 1.96 insn per cycle (65.90%) 74,065,473 branches:u # 496.187 M/sec (64.70%) 898,795 branch-misses:u # 1.21% of all branches (65.13%) 93,099,936 L1-dcache-loads:u # 623.704 M/sec (54.92%) 4,197,791 L1-dcache-load-misses:u # 4.51% of all L1-dcache hits (24.98%) 323,876 LLC-loads:u # 2.170 M/sec (29.76%) 31,341 LLC-load-misses:u # 9.68% of all LL-cache hits (42.03%) 2.348637133 seconds time elapsed [kondor6c@horse ~]$ perf stat -d urxvt urxvt: no visual found for requested depth 256, using default visual. urxvt: no visual found for requested depth 256, using default visual. Performance counter stats for 'urxvt': 221.960133 task-clock:u (msec) # 0.096 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 7,553 page-faults:u # 0.034 M/sec 283,231,245 cycles:u # 1.276 GHz (53.93%) 464,723,521 instructions:u # 1.64 insn per cycle (64.73%) 108,938,037 branches:u # 490.800 M/sec (63.29%) 1,682,643 branch-misses:u # 1.54% of all branches (61.20%) 133,799,730 L1-dcache-loads:u # 602.810 M/sec (58.34%) 4,321,438 L1-dcache-load-misses:u # 3.23% of all L1-dcache hits (26.90%) 832,193 LLC-loads:u # 3.749 M/sec (29.48%) 122,159 LLC-load-misses:u # 14.68% of all LL-cache hits (41.89%) 2.315625428 seconds time elapsed [kondor6c@horse ~]$  I ran ps auxf as quick as I could, far from scientific. But the results are interesting. I did add a few color settings to .Xdefaults (solarized and tab support).\nIn short, try to think about other tools at our disposal, we are fortunate to not have a few tools that a vendor distributes, instead many great people have encountered problems and have written tools that we can use. I try to avoid top output unless I\u0026rsquo;m in super quick triage kind of situation, where I need to understand what is going on in a disaster. Additionally plan ahead because I don\u0026rsquo;t believe that perf is installed by default on many distributions.\n{{tag\u0026gt;urxvt Desktop}}\n"
},
{
	"uri": "https://kondor6c.github.io/posts/hurricane_matthew_weekend/",
	"title": "Hurricane Matthew Weekend",
	"tags": ["linux"],
	"description": "Work that I did over when Hurricane Matthew gently blew in",
	"content": "As rain falls from Hurricane Matthew, I can\u0026rsquo;t split wood or I have started to work on some items, and have recorded various thoughts.\nI started to look at the failure of the btrfs volume, this truly appears that the failure was not hardware related; instead it is just corrupt checksums. I ran two SMART tests on all of my hard drives, none of them (I paid particular attention to the drives mentioned that were corrupt) had failed any tests. While SMART is far from perfect, it is the best kind of test that we really have at our disposal and I appreciate having it and smartmontools available. I do not believe I can, in good conscience recommend btrfs, even though I have love for open source technology.\nQuassel IRC is a nice due to the fact that I can detach graphical clients and preview links, I wish there was a something similar for BitTorrent. I have liked Qbittorrent, I can easily download magnet links and jump the the completed file to listen to it immediately. But with rtorrent you can keep it centralized. I think I am leaning in that direction since there appear to be organization methods with rtorrent so you can have for example \u0026ldquo;bitlove podcasts\u0026rdquo;, \u0026ldquo;audiobooks\u0026rdquo;, \u0026ldquo;fan fiction\u0026rdquo;, \u0026ldquo;latest version\u0026rdquo; and maybe \u0026ldquo;archive\u0026rdquo; for images such as Damn Vulnerable Linux, which is hard to obtain any more. I\u0026rsquo;m having a difficult time having files just scattered.\nFedora seemed to have had a failure with their mirrors, strangely I tried to do a dnf|yum update, and it failed due to a 503; I had never seen that before. On IRC user \u0026ldquo;misc\u0026rdquo; reported at 13:16:08, that it was a failed firewall upgrade. Additionally user smooge said that 90% of Fedora\u0026rsquo;s network is the same provider. I thought about it, I don\u0026rsquo;t think more than one provider would have prevented this unless it was a failure by the provider itself. If this was a failed upgrade done by fedora staff/volunteers then it still might have failed if a provider was being used.\nWriting some of the scripts I plan on using, I was using KDE\u0026rsquo;s Kate editor. I can\u0026rsquo;t say I like the new look and feel compared to KDE version 4. I use the vi mode, when you do a search and replace it puts you in to the vi mode, which genuinely I am moderately used to using. I know I need to get better at it, but sed works so well. I will need to start tagging blog entries, this is the longest I have blogged and information management like this is posing some new challenges. All in all, I like writing, since I have moved I don\u0026rsquo;t have many //friends// that share an interest in open source technology or what not, so I think this is an outlet to release that, so to speak.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/options-vs-arguments-vs-parameters/",
	"title": "options-vs-arguments-vs-parameters",
	"tags": ["linux"],
	"description": "trivial discussion about semantics of positioning vocabulary",
	"content": "====== options, arguments, or parameters ======\nWhere I am currently employed there has been, in my opinion, an over application of the term \u0026ldquo;parameter\u0026rdquo;. Or at least from where I have been in the past. I have always thought that for example:\nexample -x test.txt  -x would be a switch, and the option after -x, test.txt, would be argument. But it seems here or recently we apply the term parameter to everything. Which I guess I\u0026rsquo;m not opposed to, I\u0026rsquo;m just observing. Is this trivial, yes, very trivial. But I\u0026rsquo;m not really trying to advocate change or anything like that. Let\u0026rsquo;s look at the man page for sort:\nSORT(1) User Commands SORT(1) NAME sort - sort lines of text files SYNOPSIS sort [OPTION]... [FILE]... sort [OPTION]... --files0-from=F DESCRIPTION Write sorted concatenation of all FILE(s) to standard output. With no FILE, or when FILE is -, read standard input. Mandatory arguments to long options are mandatory for short options too. Ordering options: -b, --ignore-leading-blanks ignore leading blanks -d, --dictionary-order consider only blanks and alphanumeric characters -f, --ignore-case fold lower case to upper case characters -g, --general-numeric-sort compare according to general numerical value -i, --ignore-nonprinting consider only printable characters -M, --month-sort compare (unknown) \u0026lt; 'JAN' \u0026lt; ... \u0026lt; 'DEC' -h, --human-numeric-sort compare human readable numbers (e.g., 2K 1G) -n, --numeric-sort compare according to string numerical value  The man page says sort, but we can\u0026rsquo;t seem to find anything about \u0026ldquo;switch\u0026rdquo; or \u0026ldquo;parameter\u0026rdquo;. If we look at, for example, grep (perhaps one the most commonly used tools in my toolkit), swtich appears, look at the [[http://git.savannah.gnu.org/cgit/grep.git/tree/doc/grep.in.1#n294|source]]. I could find parameter but it was only in regards to awk, it was not in grep, sed, ping, awk, time, top. I did find it in [[https://github.com/curl/curl/blame/master/docs/curl.1#L773|curl]]! Here is a good a good page, describing that there is a difference in computer science in the terms: http://stackoverflow.com/questions/156767/whats-the-difference-between-an-argument-and-a-parameter If we look at [[https://en.wikipedia.org/wiki/Command-line_interface#Arguments|wikipedia]], it seems to use all of the terms. Perhaps this really just highlights the triviality of all this :-) Why did I search for this? Curious I suppose. Have a good day!\n"
},
{
	"uri": "https://kondor6c.github.io/posts/give_emacs_another_try/",
	"title": "Emacs",
	"tags": ["emacs"],
	"description": "I revisit emacs, and try to use it",
	"content": " emacs Those that know me, know that I preach the use of vi as an editor. Reason being as those who know me might be able to recite: vi is everywhere and doesn\u0026rsquo;t need to be installed, its very powerful for its size, some jobs demand it! That fact was mentioned to me by someone at Phoenix Linux Users Group meeting, which was call CAT (Chat About Technology; lets go eat CAT, it was a bad joke, but we went with it) someone stated that some GE jobs require the use knowledge of vi, sure enough I went to http://www.ge.com/careers and I found in fact some jobs do require it. I later speculated while at NightHawk, that some GE medical modalities only had vi installed. In other jobs, I witnessed administrators stopping work on solving a problem to install an editor that they could use, literally blew my mind. That\u0026rsquo;s why I try to inform people that they //at least// need some kind of working knowledge of vi and nano, but not so much nano since the editor has a command legend at the bottom.\nThis brings me to the topic of this blog entry I think that I\u0026rsquo;m going to revisit emacs, considering it hit it version 25. Honestly my very first editor was emacs, I would always press F10 and a drop down menu would be presented where I could function. This was back in Redhat 6 on my 486\u0026rsquo;s, I didn\u0026rsquo;t know much then and had trouble finding the assistance I needed, but emacs more than helped. Now I used to give emacs a hard time about the key sequences, but with F10 that can be managed. But I have come to try and use them more often since bash by default uses emacs like shortcuts. Additionally, aside from the large size of the editor (sometimes up to 120MB installed) it isn\u0026rsquo;t installed everywhere, emacs is a fine editor behaves exactly as you would expect. I have learned about emacs has something called https://www.emacswiki.org/emacs/TrampMode mode, which basically allows it to run locally and send all the changes to the files over SSH. I\u0026rsquo;m not sure I would do all my editing that way, but I could do some editing of Chef cookbooks. I have tried to use org mode, but I think I have finalized on task warrior, more on that later; I have a task entry for it! Lastly emacs has https://www.emacswiki.org/emacs/Evil, which allows for it to act like vi in some aspects.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/standardization/",
	"title": "standardization",
	"tags": ["linux"],
	"description": "small thoughts on standardization and internal rules/policies",
	"content": "====== Standardization ====== I am sure many of us have seen the xkcd comic \u0026ldquo;Standrads\u0026rdquo;, if not check it out, its kinda funny. I have been thinking about the debate and later burden that we seem to impose on ourselves through well intended standardization. This came around when there was a discussion at my full time employer about standardizing a spaces around the text and brackets only_if {node.fqdn.upcase =~ 'PROD'} compared to only_if { node.fqdn.upcase =~ 'PROD' }\nYes, I believe that the former (spaces) is easier to read. I objected to making a rule about it and burying it somewhere in a wiki that we maintain. I feared it would be a rule that would later become something that would be held against a new team member or contributor; \u0026ldquo;oh you didn\u0026rsquo;t adhere to our 8 pages of rules\u0026rdquo;. I felt it increased the barrier to entry and wasted time, not only in what we would spend maintaining the document, discussing it, possibly revising it, and the very time we spent debating the merits of having a rule. I thought it was very trivial that if we felt it was that bad to read the author should be able to do as he/she pleases. Rules and regulations are rigid in nature and tend to get buried, lost and forgotten; they grow old and start to become a burden on the organization!\nI had thought about some of these rules that Unix must have adopted in a https://utcc.utoronto.ca/~cks/space/blog/unix/MoreAndUnixFossilization about the utility \u0026lsquo;more\u0026rsquo; and \u0026lsquo;less\u0026rsquo;. Whereas they don\u0026rsquo;t mention any kind of discussion I can only think about the hours lost making the specification over something rather trivial such as a screen pager. Maybe I\u0026rsquo;m over-trivializing it? I\u0026rsquo;m not sure, but certainly in a modern perspective it feels like a minute detail. I worry that any rule I would put in place might become something like saying \u0026lsquo;more\u0026rsquo; can only do these things. Even while the tool set is rapidly advancing.\nHaving said all that, I am a huge fan of RedHat Enterprise Linux and CentOS, which to many would complain that its PHP is X years old (to which I will typically counter that their OS X BASH version is close to 10 years old, and doesn\u0026rsquo;t support named indexes/hashes). I feel with RHEL they quickly change packages and more are available through the community with https://fedoraproject.org/wiki/EPEL. They don\u0026rsquo;t want to introduce a change that might change the behavior of something that is compiled on it. The operating system is a very stable piece of software. Linus even has a rule that the kernel must never http://yarchive.net/comp/linux/gcc_vs_kernel_stability.html. I don\u0026rsquo;t know if CentOS/RHEL has a large rule set, it might break my heart if they did, but I would understand why it would be large (and hopefully lively document).\nTherefore in conclusion, I still hold my opinion on what rules and regulations are in software. They are global variables. But they have their use, I think they should be used with care like a house plant, too many of them and they become unmanageable, a burden themselves and hard to work around. With care, they become a lively addition to any organization.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/powershell_linux_debut/",
	"title": "powershell_linux_debut",
	"tags": ["linux"],
	"description": "Powershell was announced for Linux, I take a quick look",
	"content": " Powershell Linux debut I began to experiment with Powershell. I feel that Microsoft has to follow through on showing the community that it meant what it said, that Microsoft Loves Linux. Perhaps I can lump myself into that group too, because I remember the October Document, and the utter frustration in trying to get Linux to work in a mostly Windows environment. I think Microsoft senses that the game is changing, and is changing with it. Linus Torvalds had said that \u0026ldquo;If Microsoft ever does applications for Linux it means I\u0026rsquo;ve won.\u0026rdquo; and now that they are changing their business practices to bring software to Linux, is totally incredible. Its good as a company they are becoming a software company that makes software for products in demand.\nWhile I am writing this I am trying to use Powershell to some degree, my family members and friends/co-workers have told me to look into Powershell (I think mostly to assist with scripts ;-) but still). My initial observations is that everything is well laid out, with thought, Get-, Invoke-, Set-* (you get the idea). However as a shell, something you interact with frequently just to get a look at things it makes features such as tab completion very hard since they\u0026rsquo;re all the same structure. I see both sides, it being good for a consistent theme, but it gets extreme. Really what it feels like is more of an object oriented language shell. Yeah, kinda awkward. I would like to revisit this awkward-ness and either support it with more evidence or\nPS /root\u0026gt; type ./.ssh/ type : Access to the path '/root/.ssh/' is denied. At line:1 char:1 + type ./.ssh/ + ~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : PermissionDenied: (/root/.ssh/:String) [Get-Content], UnauthorizedAccessException PS /root\u0026gt; cat .ssh /bin/cat: .ssh: Is a directory PS /root\u0026gt;  hmmm seems like type should handle a directory? ls -l and find show me its a directory, but type is link a symlink to Get-Content. I could see this causing problems, especially since type is a builtin of BASH. I don\u0026rsquo;t know off the top of my head why Get-Content would be effectively symlinked.\n PS /root\u0026gt; Get-Process -Name rtorrent Get-Process : Cannot find a process with the name \u0026quot;rtorrent\u0026quot;. Verify the process name and call the cmdlet again. At line:1 char:1 + Get-Process -Name rtorrent + ~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (rtorrent:String) [Get-Process], ProcessCommandException + FullyQualifiedErrorId : NoProcessFoundForGivenName,Microsoft.PowerShell.Commands.GetProcessCommand PS /root\u0026gt; ps aux |grep rtorrent ec2-user 26164 0.0 1.0 658724 10488 pts/1 Sl+ Aug03 21:36 rtorrent PS /root\u0026gt;  It worked on systemd.\nGranted this is an alpha version, I found it hard to get \u0026ldquo;help\u0026rdquo; on this item using \u0026ndash;help, -help, /help did not\u0026hellip; help! Later learned that there is a facility entirely to this Get-Help (aka man!). Also consider that powershell will act as coreutils + more, as in it won\u0026rsquo;t have bc, wget/curl, vi, tar, or others. So powershell needs to include many these functions with itself; Invoke-RestMethod, this will come in handy later! We can set alias\u0026rsquo; but I thought some of the alias\u0026rsquo; I would be overwriting Linux utilities and then I would be in a world of pain (Yes I could specify the full path, but still not fun).\nIn conclusion this is a good thing, I don\u0026rsquo;t want my negativity to make it sound like anything bad. But I could never imagine using this as any kind of a default shell any time soon or ever. I haven\u0026rsquo;t actually got to learn that much about it, I\u0026rsquo;ve done more typing than learning.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/modern-marvels-engineering-disasters/",
	"title": "",
	"tags": ["linux"],
	"description": "",
	"content": "Engineering Disasters  \u0026quot;If you look at the major disasters in history, you see in retrospect a sequence of mistakes each one of \u0026gt;which at the time appeared to be innocent an inconsequential. Which all combined in an unfortunate way at the \u0026gt;time of the disaster to create a disaster.\u0026quot; Dr. Roger L. McCarthy PE (Chairman Exponent Failure Analysis) While watching Modern Marvels, I have been really enjoying the Engineering Disasters, the particular enjoyment lies in the fact that one can view the root cause analysis. The software engineering profession is in comparison to other engineering disciplines, such as civil engineering or aeronautical engineering is rather young (and some argue \u0026lt;http://www.theatlantic.com/technology/archive/2015/11/programmers-should-not-call-themselves-engineers/414271/\u0026gt; not a real engineering discipline, since the IEEE in some form recognizes software engineering I will too). Additionally there is little to no serious fallout from a miss of a route distribution or a variable not being in scope correctly. Of course I would like to place emphasis on the fact that some circumstances this is not true, such as the PATRIOT missile system and the time issue, or perhaps a healthcare company. There are other areas that my \u0026quot;serious fallout\u0026quot; statement does not apply, but they are the exception. If someone can't read a news site or view an advertisement everyone is still safe.\nWhat I keep thinking is, \u0026quot;what will our disasters be, if we had a software engineering disaster?\u0026quot;, there might be some outages on there and it might be very interesting. But more importantly what have we learned and as an industry what have we agreed on or put in place to prevent further disasters? If I had to immediately come in and say a few items backups and security items would be among the first. Perhaps I will make a more through list, but really as an industry as we mature I think we will arrive on more through measures. But since our industry is advancing so rapidly (far more quickly than civil engineering for example, please don't take offense, my grandfather is a civil engineer and I have great respect for him and his practice), I think that if we ratified a standard, it would quickly become out of date and would instead hold us down. This is especially true with security.\nSo really, maybe, our industry is unique and young, I really look forward to how it will change! But I also think we will increasingly need to be more accountable for our mistakes and acknowledge them (I also think this is a sigh of maturity in general in particular with junior and senior level engineers). Fortunately we won't be liable for a dam collapse which would kill hundreds, or a structural engineer that has a pillar collapse and thus be brought to trial for the failure. Keep in mind, what small things that we think are uncommon and just \u0026quot;won't happen\u0026quot; might cause a failure somewhere else. With those thoughts and planning, we will be better engineers! Try reading the book Web Operations \u0026lt;http://shop.oreilly.com/product/0636920000136.do\u0026gt; or watch a hangops video, of which I will correctly link later.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/btrfs_total_failure/",
	"title": "RAID5 btrfs total failure and subsequent abandonment",
	"tags": ["linux", "btrfs"],
	"description": "",
	"content": "RAID5 btrfs total failure and subsequent abandonment My btrfs volume failed last night. I tried to rebalance the array to a \u0026quot;JBOD\u0026quot; (just a bunch of disks), it seems that it failed during that rebalance. To really drill down into the failure, it seems based on lines of the device info (bdev...) both those devices have errors. I suspect that these one batch of these read errors were directly due to the failure last time, (this one has rd 76 corrupt 16, other one had corrupt 8 and read 76). Two \u0026quot;failed\u0026quot; devices, during the course of a rebalance, where I was trying to change the structure of the file system. I think I screwed myself over. I would have thought, based on my last rebalance that it would have refused to run, due to fact that there were bad checksums/errors on the device. I have remembered that btrfs took a very long time to get a check/repair ability (fsck), and that was one reason why ext4 was used in Fedora 24 \u0026lt;https://www.reddit.com/r/linux/comments/3awppp/btrfs_as_default_filesystem_for_fedora_23/\u0026gt;, although OpenSUSE has made it the default.\nMy next filesystem for my movie backups, Virtual Machine images, and other files, will probably be ZFS. I remember being told about ZFS back in 2006. I remember just being awe of that filesystem, mostly the fact that it had RAIDz, compression, checksumming, I didn't know a lot about the details of it but it sounded like a comprehensive filesystem. I was wanting to try it but never had the ability to use it, and then I tried to purchase spare Solaris machines on eBay. I was told never to even look at the source code of ZFS or OpenSolaris due to CDDL (I'm not a lawyer, I know the man's intent was good, but it might have been an exaggeration). But this isn't stopping me from trying to run it on Linux now, one difference that I'm not looking forward to is that there is no defragmentation support (neither offline or online, just doesn't exist), and there is no ability to change the file system type like you can with btrfs (even though that is the part that seemed to fail on me). This file system is going to be a secondary file system on my machine, so it being a tainted 3rd party kernel module is not a concern to me. Also another point is that you need ECC memory, which I have on this machine. Considering all that I should be good with ZFS. I will try to post my experiences with it.\nHere is the actions I took and most of the output, if anyone comes to this post from a search engine, my volume failed I did not recover the data: .. code-block:: console\n [root\u0026#64;horse ~]# btrfs balance start -dconvert=single /mnt/extra/ ERROR: error during balancing '/mnt/extra/': Read-only file system There may be more info in syslog - try dmesg | tail [root\u0026#64;horse ~]# [root\u0026#64;horse ~]# dmesg | tail [ 3710.919178] \u0026nbsp;[\u0026lt;ffffffff8f1f4d3d\u0026gt;] ? __vma_link_rb+0xfd/0x110 [ 3710.919182] \u0026nbsp;[\u0026lt;ffffffff8f25b4b2\u0026gt;] do_vfs_ioctl+0xa2/0x5d0 [ 3710.919186] \u0026nbsp;[\u0026lt;ffffffff8f062776\u0026gt;] ? __do_page_fault+0x206/0x4d0 [ 3710.919190] \u0026nbsp;[\u0026lt;ffffffff8f25ba59\u0026gt;] SyS_ioctl+0x79/0x90 [ 3710.919196] \u0026nbsp;[\u0026lt;ffffffff8f7ec572\u0026gt;] entry_SYSCALL_64_fastpath+0x1a/0xa4 [ 3710.919228] ---[ end trace 58ee1a8a51b783ac ]--- [ 3710.919232] BTRFS: error (device sdf) in btrfs_run_delayed_refs:2963: errno=-5 IO failure [ 3710.919235] BTRFS info (device sdf): forced readonly [ 3734.949833] BTRFS info (device sdg1): disk space caching is enabled [ 3734.949839] BTRFS info (device sdg1): has skinny extents [ \u0026nbsp; 71.460425] BTRFS info (device sdb): disk space caching is enabled [ \u0026nbsp; 71.460433] BTRFS info (device sdb): has skinny extents [ \u0026nbsp; 72.576302] BTRFS info (device sdb): bdev /dev/sdd errs: wr 0, rd 76, flush 0, corrupt 16, gen 0 [ \u0026nbsp; 72.576311] BTRFS info (device sdb): bdev /dev/sdf errs: wr 510, rd 158743, flush 170, corrupt 0, gen 0 [ \u0026nbsp; 78.476016] BTRFS error (device sdb): parent transid verify failed on 12670714019840 wanted 95664 found 90828 [ \u0026nbsp; 78.488997] BTRFS error (device sdb): parent transid verify failed on 12670714019840 wanted 95664 found 90828 [ \u0026nbsp; 78.489007] BTRFS error (device sdb): failed to read block groups: -5 [ \u0026nbsp; 78.507884] BTRFS: open_ctree failed [root\u0026#64;horse ~]# mount -o degraded /dev/sdf /mnt/extra/ mount: wrong fs type, bad option, bad superblock on /dev/sdf, \u0026nbsp; \u0026nbsp; \u0026nbsp; missing codepage or helper program, or other error \u0026nbsp; \u0026nbsp; \u0026nbsp; In some cases useful info is found in syslog - try \u0026nbsp; \u0026nbsp; \u0026nbsp; dmesg | tail or so. [root\u0026#64;horse ~]# [root\u0026#64;horse ~]# btrfsck --repair --init-csum-tree /dev/sdb enabling repair mode Creating a new CRC tree parent transid verify failed on 12670714019840 wanted 95664 found 90828 parent transid verify failed on 12670714019840 wanted 95664 found 90828 parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure leaf parent key incorrect 12670714019840 Checking filesystem on /dev/sdb UUID: 8289c25f-a8d1-44aa-8c18-6215831d2cae Reinit crc root parent transid verify failed on 12670694375424 wanted 95645 found 90828 parent transid verify failed on 12670694375424 wanted 95645 found 90828 parent transid verify failed on 12670694375424 wanted 95645 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670713872384 wanted 95664 found 90828 parent transid verify failed on 12670713872384 wanted 95664 found 90828 bytenr mismatch, want=12670713872384, have=12670713741312 parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670714019840 wanted 95664 found 90828 Ignoring transid failure parent transid verify failed on 12670713872384 wanted 95664 found 90828 parent transid verify failed on 12670713872384 wanted 95664 found 90828 bytenr mismatch, want=12670713872384, have=12670713741312 Unable to find block group for 0 extent-tree.c:289: find_search_start: Assertion 1 failed. btrfs check(+0x4e29a)[0x556fccd8d29a] btrfs check(btrfs_reserve_extent+0xabe)[0x556fccd9215e] btrfs check(btrfs_alloc_free_block+0x5f)[0x556fccd9221f] btrfs check(+0x46584)[0x556fccd85584] btrfs check(btrfs_search_slot+0x2a2)[0x556fccd863c2] btrfs check(btrfs_csum_file_block+0x47f)[0x556fccd9786f] btrfs check(+0xf67e)[0x556fccd4e67e] btrfs check(cmd_check+0x27be)[0x556fccd72d4e] btrfs check(main+0x7d)[0x556fccd4eddd] /lib64/libc.so.6(__libc_start_main+0xf1)[0x7fb048cd4731] btrfs check(_start+0x29)[0x556fccd4eee9] [root\u0026#64;horse ~]# "
},
{
	"uri": "https://kondor6c.github.io/posts/bash_tips1/",
	"title": "bash tips 1",
	"tags": ["linux"],
	"description": "after reviewing some &#34;bash tips&#34; I post a reflection",
	"content": " BASH tips So I read a comment on Hacker News about ctrl+o and and the comment (and subsequent comments) made it sound as though it was this ultra helpful trick. Well I had a bit of difficulty finding what and how to use, perhaps its because I\u0026rsquo;m a bit stupid. So here is how it works, you type a command (I found it works best when you use ctrl+r then press ctrl+o) press ctrl+o instead of enter. It will run that command, then autocomplete the next command from the last previously typed entry that you ran when you pressed ctrl+o. Might be a little confusing. Take a look at this fake, yet focused on the concept session below\n[kondor6c@workhorse ~]# \u0026lt;CTRL+R\u0026gt; service tomcat sto\u0026lt;CTRL+O\u0026gt; Redirecting to /bin/systemctl stop tomcat.service [kondor6c@workhorse ~]# rsync -va current-version.war server:/app/tomcat/webapps/ #this will automatically appear! Just press to get the next line to automatically appear\u0026lt;CTRL+O\u0026gt; kondor6c@server.mooo.com's password: sending incremental file list current-version.war sent 43,611,957 bytes received 824 bytes 3,426.29 bytes/sec total size is 43,611,838 speedup is 0.99 [kondor6c@workhorse ~]# service tomcat start \u0026lt;enter\u0026gt; Redirecting to /bin/systemctl start tomcat.service  As you can see ctrl+o would require explicit knowledge of what was ran last, otherwise you wasted time. Additionally I would want to group any commands that I would type after one another into a one liner or make it a script-able action. In short I\u0026rsquo;m glad I learned about it, I probably won\u0026rsquo;t instruct others about it, as I think it will be harder to grasp, and I don\u0026rsquo;t see the use case being that strong.\nI have explored many \u0026ldquo;Bash tips\u0026rdquo; I think I know the vast majority of them. If you know of one I encourage you to share it. Here are just a couple that I have very recently discussed with coworkers/intern:\n \u0026ldquo;ctrl + \\\u0026rdquo; : sends a SIGQUIT as in a signal 3, which tries to dump the core, if not then it will generally quit with a higher priority than SIGINT, not really a BASH specific tip, but certainly useful. \u0026ldquo;~.\u0026rdquo; : When ssh\u0026rsquo;ing and you have a hung session, press \u0026ldquo;~.\u0026rdquo; on the SSH session, this should interrupt it and bring you out. I have found that when my sessions are hung they are not always at a newline  kondor6c@computer:~$ ~? Supported escape sequences: ~. - terminate connection (and any multiplexed sessions) ~B - send a BREAK to the remote system ~C - open a command line ~R - request rekey ~V/v - decrease/increase verbosity (LogLevel) ~^Z - suspend ssh ~# - list forwarded connections ~\u0026amp; - background ssh (when waiting for connections to terminate) ~? - this message ~~ - send the escape character by typing it twice (Note that escapes are only recognized immediately after newline.) kondor6c@computer:~$ #\u0026lt;enter\u0026gt; kondor6c@computer:~$ #\u0026lt;enter\u0026gt; then press ~. kondor6c@computer:~$ Connection to computer closed.  "
},
{
	"uri": "https://kondor6c.github.io/posts/white_spaces_can_be_dangerous/",
	"title": "white_spaces_can_be_dangerous",
	"tags": ["linux"],
	"description": "brief look at go and spacing",
	"content": "====== White spaces can be dangerous ====== When I was some go I noticed that it refused to compile due to a white space:\n\u0026ldquo; ./go-test-router.go:17: invalid identifier character U+2502 ./go-test-router.go:17: syntax error: unexpected name, expecting } \u0026ldquo;\nI have noticed this first with Microsoft Windows formatting of white spaces and new lines. This is also true Word quotes. This was due to the fact that we had to put documentation on sharepoint. Something I still don\u0026rsquo;t agree with to this day.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/tiddlywiki_entry_everyday_for_30_days/",
	"title": "tiddlywiki_entry_everyday_for_30_days",
	"tags": ["linux", "note_taking"],
	"description": "I try to commit to writing in tiddlywiki once a day for 30 days",
	"content": "====== Tiddlywiki entry everyday for 30 days ======\nI\u0026rsquo;m going to make it a goal to make a tiddlywiki entry every day for 30 days. This will be in effort to help form a habit. Here is my plan, add items to track quick ideas like Million dollar ideas, things to research, plans for the house, programming/learning notes. Really Tiddlywiki should be quick and dirty, I want this dokuwiki to be more mature, organized, especially since it\u0026rsquo;s versioned with git. With that I can more or less commit myself to goals (even though the goals section is just as \u0026ldquo;set in stone\u0026rdquo; as this is). I need to tiddly for 30 days, wiki 30 days, blog 30 days, keepass 30 days. Physically, I should stationary bike for 30 days, lift weights/push ups 30 days, run 30 days. Here are some blog entry ideas: * thoughts on RPM macros, Fedora installing them now with an RPM * thoughts on new KDE/plasma and the work laptop * reflect on 30 day progresses * golang efforts I can\u0026rsquo;t think of any more right now, but I should put them in Tiddlywiki. By the way, Ubuntu keeps the trackpad enabled whereas Fedora has it insensitive. I\u0026rsquo;ve already highlighted and deleted this text\u0026hellip; This needs to change.\n"
},
{
	"uri": "https://kondor6c.github.io/posts/x2go_or_freenx/",
	"title": "x2go or FreeNX",
	"tags": ["community", "fedora", "qa", "remote"],
	"description": "",
	"content": "x2go or freenx I have had the worst time with x2go both on Fedora and Ubuntu, I thought neatx from Google was the preferred method for doing remote sessions but I heard that had changed. x2go doesn't even offer a Windows client like NoMachine offers. I was trying to test libpng on a scaleway Fedora 22 ARM server (https://bodhi.fedoraproject.org/updates/FEDORA-2015-6c07ab1fa6). I guess I'll have to find another way of doing that.\n"
},
{
	"uri": "https://kondor6c.github.io/tls_in_practice/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Trust with TLS Date: Wed, 03 Feb 2019 00:54:56 -0400 modified:Wed, 03 Feb 2019 00:54:56 -0400  category:docs, features  tags:comprhensive  slug:Trust_with_TLS  Authors: Kevin Faulkner summary:An expanding document that should fully describe many components to TLS, geared for quick application    Trust! What is trust? It really is the common name for the measure of authenticity, mainly testing the location you are trying to connect to. The standard by which something is verified to be genuine. The verification of the client is really more of authentication, user/client. The two are very similar, an authentic which is probably why they both share the same root name, authentikos greek meaning approximately original, primary one. In modern day computing, we do this by trusting a few locations (Root CA's) then they issue trust to Intermediates who can then extend that trust to other locations like *.lwn.net or a specific name mail.kernel.org. I will try to explain all of this and more. It should be through, and I hope I explain it well. The focus of this document has been geared towards copy and pastable commands, this is so that busy/jr enegineers/administrators can copy and paste and get a mildly working setup. In some places input or specifying a different option is required.\nOpenSSL optional variables, for cleanliness purposes Generally speaking this will be the style throughout the document Root-Intermediate-\u0026gt; will be for server certificates Root-Intermediate-Intermediate-\u0026gt; will be for a cool team, demonstrating that we can have multi-level intermediates. This path is similar to RFC 4135 2.1\nRoot-IntermediateUsers is User authority The files will typically have this pattern Intermediate-${cipher}_,${user},${note}.${type} Root-${cipher}_${Server},${note}.${type} Here are the filenames I use, I'm pretty bad at naming things, but something is better than nothing.\nexport CA_PATH=$(pwd)/common_infra export CERT_PATH=${CA_PATH}/certs export KEY_PATH=${CA_PATH}/keys export CSR_PATH=${CA_PATH}/csr export SRL_PATH=${CA_PATH}/serial export REVOKE_PATH=${CA_PATH}/revoke mkdir -p ${CSR_PATH}/ ${KEY_PATH}/ ${CERT_PATH}/ ${REVOKE_PATH}/ ${SRL_PATH}/  RootCA ECC generate the key, then generate the request, otherwise known as the Certificate signing request. The req request takes the request and immediately signs it and generates the certificate. Root certicates are always self signed, these are the highest level of the trust tree. Feel free to give this certificate out. If you own this machine, distribute it, and trust it (especially in a lab/sandbox).\nopenssl ecparam -genkey -name prime256v1 -out ${KEY_PATH}/RootCA_ECC.key openssl req -new -x509 -sha256 -days 2191 -key ${KEY_PATH}/RootCA_ECC.key -reqexts v3_req -extensions v3_ca -out ${CERT_PATH}/RootCA_ECC.crt  Note\nOptional, we can do these operations as different commands, if you are learning this might be a good route, just to see what the command above is doing.\n openssl req -new -sha256 -nodes -key ${KEY_PATH}/RootCA_ECC.key -reqexts v3_req -out ${CSR_PATH}/RootCA_ECC.csr \u0026amp;\u0026amp; \\ openssl x509 -req -sha256 -signkey ${KEY_PATH}/RootCA_ECC.key -in ${CSR_PATH}/RootCA_ECC.csr -extensions v3_ca -out ${CERT_PATH}/RootCA_ECC.crt  Optional (generates the revoke list) Here a certificate revoke list is being generated. A certificate revoke list a location where clients can check to see if the issuer has revoked the certificate they are about to consume. These are typically not used day-to-day, and OCSP (stapling) is favored. Since OCSP prevents SSL Stripping, which can be done by proxies. If you want something funny, look at the bug of man openssl-crl\nopenssl crl -inform PEM -in ${KEY_PATH}/RootCA_ECC.key -CAfile ${CERT_PATH}/RootCA_ECC.crt -outform DER -out ${REVOKE_PATH}/RootCA_ECC.crl    IntermediateCA ECC These step could be repeated for a client certificate authority; it might be good so that this CA handles only users and will be need to be explicitly added to which ever trust will be handling the users. Additionally this could be given to an intern or contractor, and if the CA key/secret is compromised, there is limited impact to other CA's.\nopenssl ecparam -genkey -name prime256v1 -out ${KEY_PATH}/IntermediateCA_ECC.key openssl req -new -sha256 -nodes -key ${KEY_PATH}/IntermediateCA_ECC.key -out ${CSR_PATH}/IntermediateCA_ECC.csr #CSR's openssl x509 -req -days 1200 -sha256 -in ${CSR_PATH}/IntermediateCA_ECC.csr -CAkey ${KEY_PATH}/RootCA_ECC.key -CA ${CERT_PATH}/RootCA_ECC.crt -out ${CERT_PATH}/IntermediateCA_ECC.crt -CAcreateserial -CAserial ${SRL_PATH}/IntermediateCA_ECC.srl #optional: -set_serial 01  CA Answers If you want to make an \u0026quot;answers\u0026quot; file, this will allow you to by pass many of OpenSSL's prompts. When dealing with a lot of certificates this is very useful. Here is one for a Certificate Authority. Since by now you've already worked through a prompt with the Root CA, you should pretty much understand what the fields are used for.\n[req] prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C=US ST=North Carolina O=LazyTree localityName= commonName=*.lazytree.us organizationalUnitName=HomeLab emailAddress=kondor6c\u0026#64;lazytree.us  Here we'll generate a server certificate with the same encryption type. But we'll do something a little special here. We'll specify extensions to the X509 certificate types. These are added on top of the X509 certificates, the really improve things good deal and chances are you'll need them, almost 100% you'll need Subject Alternate Names, typically just called \u0026quot;SANs\u0026quot;. The following is pretty much copy pastable, if you are in a pinch grab and go, replace some of the unique fields like file names and Locality type repsonses.\nopenssl ecparam -genkey -name prime256v1 -out wildcard_lazytree_ECC.pem openssl req -new -sha256 -key wildcard_lazytree_ECC.pem -out ${CSR_PATH}/wildcard_lazytree_ECC.csr -config \u0026lt;( cat \u0026lt;\u0026lt;-EOD [req] prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C=US ST=North Carolina O=LazyTree localityName=Redacted OU=HomeLab emailAddress=kondor6c\u0026#64;lazytree.us CN=null.lazytree.us [ req_ext ] subjectAltName = \u0026#64;alt_names [ alt_names ] DNS.1 = expired.lazytree.us DNS.2 = testing.lazytree.us DNS.3 = lazytree.us EOD openssl x509 -req -days 800 -sha256 -in ${CSR_PATH}/IntermediateCA_ECC.csr -CAkey IntermediateCA_ECC.pem -CAserial RootCA_ECC.srl -out ${CERT_PATH}/wildcard_lazytree_ECC.crt   Serial The first time you use your CA to sign a certificate you can use the -CAcreateserial option. This option will create a file (ca.srl) containing a serial number. You are probably going to create more certificate, and the next time you will have to do that use the -CAserial option (and no more -CAcreateserial) followed with the name of the file containing your serial number. This file will be incremented each time you sign a new certificate. This serial number will be readable using a browser (once the certificate is imported to a pkcs12 format). And we can have an idea of the number of certificate created by a CA.\n  Clients trust This will allow clients to use certificate in a two manner there are many exampes of big projects that have support of this (but not limited to): postgres dovecot mysql HAProxy Apache nginx curl kafka    I hope the list jogs your mind on where you can take this, two way SSL or \u0026quot;mTLS\u0026quot; or Mutual Authentication is really just allowing the client (the one connecting to the server) to specify a certificate, this is done at the client portion of the TLS handshake, which we'll dig into soon. Let's go ahead and generate the client cert here. I mentioned at the beginning of this documentation that I would try to use a different Intermediate for usage as a client CA. This is because you'll typically need to distribute this CA to clients, and might need to give access to the intermediates to other teams, like a client satisfaction team or sales engineers to issue new client certs quickly. This is just an example, not a best practice.\nopenssl ecparam -genkey -name prime256v1 -out ${client_key_out} openssl req -new -sha256 -key ${client_key_out} -out ${CSR_PATH}/client_lazytree_ECC.csr openssl x509 -req -days 300 -sha256 -in ${CSR_PATH}/client_lazytree_ECC.csr -CA ${CERT_PATH}/ IntermediateClientCA_ECC.crt -CAkey IntermediateClientCA_ECC.pem -out ${CERT_PATH}/client_lazytree_ECC.crt  Chains or Bundles Chains can be used, or they don't have to be. The usage lies in the fact that if an intermediate is not trusted, but the root certificate is, or another intermediate in the chain is trusted. The name bundles are used because there are bundles of certificates (Root and Intermediates), it is highly recommended that the fully chain, be sent (hey you reading this, send dat chain!). You can find options that are used for CA Chains in the server secition below. The order is defined in RFC-5246 \u0026lt;https://tools.ietf.org/html/rfc5246#section-7.4.2\u0026gt; The order is exactly as follows: Server Certificate Intermediate \u0026lt;optional\u0026gt; another Intermediate that has signed number two Root Certificate    cat IntermediateCA.crt RootCA.crt \u0026gt; Cert-Chain.pem cat IntermediateCA_ECC.crt RootCA_ECC.crt \u0026gt; Cert-Chain_ECC.pem    Verification of Certificates It is always good to verify your work, even better to have a buddy check your work too, you never know what you might learn from somebody else's perspective.\nExamine a certificate Check your work\nopenssl x509 -in ${CERT_PATH}/certificate.crt -text -noout  Examine a key (RSA) You can also look at the key you produced\nopenssl rsa -in privateKey.key -check   Examine a Certificate Signing Request (CSR) To view a previously generated certificate signing request you can run the following.\nopenssl req -text -noout -verify -in CSR.csr    Revoke a Cert As mentioned, revokation lists and the revoking process isn't done too much. But it could really help out, consider an example, 24 hours before a certificate is about to expire if an Internal CA were to revoke the soon to expire certificate, you will have an opportunity to know for sure which applciations depend on the certifcate. This could be very useful for large organizations. Just a tip!\nopenssl ca -config ca.conf -revoke ia.crt -keyfile ca.key -cert ${CERT_PATH}/ca.crt -crl_reason superseded     Configuring SSL on Operating Systems Here is a list of operating systems and how to configure SSL on them, I hope this helps, if you know of somelet me know (open a pull request).\nWindows First we need to prep, the the best of my knowledge windows doesn't handle pem formats, which is pretty frustrating. So we need to export it to a PKCS12 format.\nopenssl pkcs12 -export -in wildcard_lazytree_ECC.crt -inkey wildcard_lazytree_ECC.pem -out wildcard_lazytree_ECC.pfx -certfile Cert-Chain_ECC.pem openssl pkcs12 -export -in wildcard_lazytree.crt -inkey wildcard_lazytree.pem -out wildcard_lazytree.pfx -certfile Cert-Chain.pem openssl pkcs12 -export -nokeys -in RootCA.crt -out RootCA.pfx openssl pkcs12 -export -nokeys -in RootCA_ECC.crt -out RootCA_ECC.pfx  Now we can take that file and add it to Windows\ncertutil.exe -addstore \u0026quot;RootCA_SHA1\u0026quot; RootCA.pfx certutil.exe -addstore \u0026quot;RootCA_ECC\u0026quot; RootCA_ECC.pfx certutil.exe -importPFX wildcard_lazytree_ECC.pfx certutil.exe -importPFX wildcard_lazytree.pfx   RHEL-like Linux You can easily add certificates to Redhat like distributions like Fedora, Centos, Amazon Linux, Scientific Linux or Oracle Linux. Consider distributing this as an RPM.\nrsync -va \\*crt /etc/pki/ca-trust/source/anchors/ update-ca-trust force-enable   Debian-like Linux AND Gentoo Gentoo \u0026lt;https://wiki.gentoo.org/wiki/Certificates\u0026gt; https://www.archlinux.org/news/ca-certificates-update/\nrsync -va \\*crt /usr/local/share/ca-certificates/ update-ca-certificates   Android Settings \u0026gt; Security \u0026amp; Lock Screen \u0026gt; Credential storage (under \u0026quot;advanced\u0026quot;) \u0026gt; Install from storage\n  Applications Java Java holds the keys and certificates in a special file, called a keystore. It used to be a proprietary format JKS, but the newer, preferred format is p12 (PKCS12). You can access it with keytool, which should be in the same path as java ($JAVA_HOME/bin/).\nkeytool -v -list -keystore /etc/pki/ca-trust/extracted/java/cacert || keytool -v -list -keystore /etc/pki/java/cacerts #changeit is Java's default keytool -import -trustcacerts -alias rootCA_ECC -file RootCA_ECC.crt keytool -import -trustcacerts -alias IntermediateCA_ECC -file IntermediateCA_ECC.crt keytool -import -trustcacerts -alias rootCA_weak -file RootCA.crt keytool -import -trustcacerts -alias IntermediateCA_weak -file IntermediateCA.crt   Chrome https://stackoverflow.com/questions/7580508/getting-chrome-to-accept-self-signed-localhost-certificate You can avoid the message for trusted sites by installing the certificate. This can be done by clicking on the warning icon in the address bar, then click \u0026quot;Not secure\u0026quot; -\u0026gt; Certificate Invalid -\u0026gt; Details Tab -\u0026gt; Export... Save the certificate.\nUse Chrome's Preferences -\u0026gt; Under The Hood -\u0026gt; Manage Certificates -\u0026gt; Import. On the \u0026quot;Certificate Store\u0026quot; screen of the import, choose \u0026quot;Place all certificates in the following store\u0026quot; and browse for \u0026quot;Trusted Root Certification Authorities.\u0026quot; Restart Chrome. Chrome Settings \u0026gt; Show advanced settings \u0026gt; HTTPS/SSL \u0026gt; Manage Certificates \u0026gt; Authorities\n Nginx https://www.digitalocean.com/community/tutorials/how-to-create-a-self-signed-ssl-certificate-for-nginx-on-centos-7\nserver { listen 80; server_name \u0026quot;example.lazytree.us\u0026quot;; return 301 https://$host$request_uri;  } server { server_name \u0026quot;10.1.1.1\u0026quot; listen 443 http2 ssl; listen [::]:443 http2 ssl; ssl_certificate /etc/ssl/certificates/example.lazytree.us/app_role.crt; ssl_certificate_key /etc/ssl/keys/example.lazytree.us/app_role.key; ssl_dhparam /etc/ssl/keys/example.lazytree.us/dhparam.pem;  }   Apache https://wiki.apache.org/httpd/RedirectSSL\nListen 443 ssl \u0026lt;VirtualHost _default_:443\u0026gt; ServerName lazytree.us SSLEngine on SSLProtocol all -SSLv2 -SSLv3 SSLCertificateFile /etc/pki/tls/certs/ SSLCertificateKeyFile /etc/pki/tls/private/ SSLCertificateChainFile /etc/pki/tls/certs/chain.crt SSLCACertificateFile /etc/httpd/conf.d/tls/client_IntermediateCA.crt SSLOpenSSLConfCmd DHParameters \u0026quot;/etc/pki/ssl/dhparams.pem\u0026quot; RewriteEngine On RewriteCond %{HTTPS} off RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L]  \u0026lt;/VirtualHost\u0026gt;  # It would be nice to get blake2s256 supported in more places #GPG fingerprint = 7545BFF3710684D2E6BCFE98C5D5F4BED24A4A02 #GPG fingerprint = 438263E03BF0BDC64F9A6415AA63E0576CC60292\n  GNU TLS I have recently been liking GnuTLS since it has rather descriptive options, they are easy to read and self describing of the process. The issue is that it isn't always installed.\ncerttool --generate-privkey --bits 4096 --outfile RootCA_G-RSA.pem certtool --generate-request --load-privkey RootCA_G-RSA.pem --hash=SHA256 --template gnutls-ssl-answers.txt --outfile RootCA_G-RSA.csr certtool --generate-certificate --load-privkey RootCA_G-ECC.pem --outfile RootCA_G-ECC.crt --load-ca-certificate ca-cert.pem --load-ca-privkey ca-key.pem  GNU TLS ECC Coming in version 3.6!! Ed25519 keys certtool --generate-privkey --key-type ed25519 --outfile RootCA_G-ECC.pem otherwise you might need to go with secp 256\ncerttool --generate-privkey --ecc --curve secp256r1 --outfile RootCA_G-ECC.pem certtool --generate-request --load-privkey RootCA_G-ECC.pem --hash=SHA256 --outfile RootCA_G-ECC.csr certtool --generate-certificate --load-privkey RootCA_G-ECC.pem --outfile RootCA_G-ECC.crt --load-ca-certificate ca-cert.pem --load-ca-privkey ca-key.pem    Quick Reference Lists You could call these \u0026quot;cheat sheets\u0026quot; but these are more translation matrixes, like a rosetta stone of options. I often get frustrated with how many different options I'm called to remember (not entirely told, but just feel as though, professionally, I should). It can be difficult not specializing in a specific peice of software, since you have a ever expanding target of defaults, types of actions, configurations locations, and command line arguements; but I digress. I hope to make more of these, checkout my dotfiles where I use the top 26 lines as a quick reference. Some the options I know, others I have a hard time remembering, while others I learned while making it. https://github.com/kondor6c/dotfiles\n verification   OpenSSL GnuTLS function   x509 -verify --verify verify x509 cert  x509 -CAfile --load-ca-certificate verify chain CA file  x509 -text -noout -in --certificate-info --infile verify an x509 cert  req -noout -text -in --crq-info --infile examine a CSR     x509 generation   OpenSSL GnuTLS function   x509 -text -noout -in --verify --infile verify x509 certicate  x509 -CAfile --load-ca-certificate verify chain againt file  x509 -CAkey --load-ca-privkey load CA key to sign  x509 -req --load-request load CSR to sign for cert  x509 -CA --load-ca-certificate Load CA cert to sign  x509 -config --template preconfigured answers  x509 -sha256 --hash=SHA256 certificate hash (sha256)     keys (RSA)   OpenSSL GnuTLS function   genrsa -out --generate-privkey --outfile Write rsa Key to file  rsa -noout -text -in -k --infile examine RSA Key     Diffie Helman   OpenSSL GnuTLS function   dhparam 2048 -out --generate-dh-params generate parameters     ref links and stuff https://jamielinux.com/docs/openssl-certificate-authority/create-the-root-pair.html\n RFC list Forward Secrecyree_SSH_CA-ed -h -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH_CA-ecdsa -h -t ecdsa -b 521 ssh-keygen -f LazyTree_SSH_CA-rsa -h -t rsa -b 4096\nTraditional Host Keys ssh-keygen -f LazyTree_SSH-rsa -t rsa -b 4096 ssh-keygen -f LazyTree_SSH-ed -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH-ecdsa -t ecdsa -b 521\n DH Params Diffie Helman is pretty cool\nopenssl dhparam -out dhparam.pem 4096    OpenSSH CA https://blog.habets.se/2011/07/OpenSSH-certificates.html\n Host CA's ssh-keygen -f LazyTree_SSH_CA-ed -h -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH_CA-ecdsa -h -t ecdsa -b 521 ssh-keygen -f LazyTree_SSH_CA-rsa -h -t rsa -b 4096\nTraditional Host Keys\nssh-keygen -f LazyTree_SSH-rsa -t rsa -b 4096 ssh-keygen -f LazyTree_SSH-ed -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH-ecdsa -t ecdsa -b 521\n  "
},
{
	"uri": "https://kondor6c.github.io/posts/tls_in_practice/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Trust TLS What is trust? It really is the common name for the measure of authenticity, mainly testing the location you are trying to connect to. The standard by which something is verified to be genuine. The verification of the client is really more of authentication, user/client. The two are very similar, an authentic which is probably why they both share the same root name, authentikos greek meaning approximately original, primary one. In modern day computing, we do this by trusting a few locations (Root CA's) then they issue trust to Intermediates who can then extend that trust to other locations like *.lwn.net or a specific name mail.kernel.org. I will try to explain all of this and more. It should be through, and I hope I explain it well. The focus of this document has been geared towards copy and pastable commands, this is so that busy/jr enegineers/administrators can copy and paste and get a mildly working setup. In some places input or specifying a different option is required.\nOpenSSL optional variables, for cleanliness purposes Generally speaking this will be the style throughout the document Root-Intermediate-\u0026gt; will be for server certificates Root-Intermediate-Intermediate-\u0026gt; will be for a cool team, demonstrating that we can have multi-level intermediates. This path is similar to RFC 4135 2.1\nRoot-IntermediateUsers is User authority The files will typically have this pattern Intermediate-${cipher}_,${user},${note}.${type} Root-${cipher}_${Server},${note}.${type} Here are the filenames I use, I'm pretty bad at naming things, but something is better than nothing.\nexport CA_PATH=$(pwd)/common_infra export CERT_PATH=${CA_PATH}/certs export KEY_PATH=${CA_PATH}/keys export CSR_PATH=${CA_PATH}/csr export SRL_PATH=${CA_PATH}/serial export REVOKE_PATH=${CA_PATH}/revoke mkdir -p ${CSR_PATH}/ ${KEY_PATH}/ ${CERT_PATH}/ ${REVOKE_PATH}/ ${SRL_PATH}/  core x509 certificates details The x509 certificate structure is very well defined. Since the native form of the certificate is in ASN1, it is often useful to view those detials, you can do that with this command: openssl x509 -in cert.pem -outform der | openssl asn1parse -inform der -i. It should output something like this: | tbsCertificate -------------- This field is also known as the \u0026quot;DN\u0026quot; and contains such fields as:\n Name of the subject (CN) Name of the issuer (CN) Public Key of the subject Expire Date Start date Version number (v2, v3) Serial number (from CA)   Extensions will be in here, since this is the core components of an x509 certificate (as specified in the header), we won't go into all the extensions and the fields they bring. It should be noted that each extension has an OID and an ASN.1, so it can be viewed in a structured manner.\nsignatureAlgorithm This is the algorithm used by the CA when the certificate is produced, and what was used to sign the digital certificate. This way it is known exactly what algorithm to use in order to decode the contents for verification. This WILL/MUST match the Signature algorithm (below) example\nSignature Algorithm: sha384WithRSAEncryption   signatureValue (RFC5280-4.1.1.3) This value is the signature (also hash) of tbsCertificate (DN, issuer, expire date, and more). By having this field, the CA has certified that the tbsCertificate field contents are valid. Primarily this is focused on the public key(link) and the subject of the certificate. Another way of putting is that by trusting the CA, you can trust that the tbsCertificate field is also valid.\n Signature This is simply the signature of all the certificate contents. Essentially it is showing that all the contents have been signed and the CA stamped the this with its signet ring and verified. If this part is a unclear, try reviewing PKI signatures.\nexample (from certtool)\nSignature Algorithm: RSA-SHA384 Signature: 42:68:44:75:0b:1a:6e:c0:f0:f7:38:b4:54:57:e1:64 00:58:d7:e5:5f:3c:94:8b:94:2e:85:2a:24:a8:48:70 4b:e4:02:1d:81:84:5d:31:52:47:c1:87:39:84:2e:4d 10:6b:cc:35:cd:64:e0:dc:de:19:fc:9f:c6:45:9c:61 ea:fc:d3:97:3e:5b:50:79:77:64:70:60:12:84:7c:91 ea:fc:ad:bc:e8:01:da:e4:39:17:a1:65:3f:ab:a6:f4 7a:6b:76:72:2a:d2:07:ec:0c:e1:f1:b6:17:20:60:a9 19:28:24:8c:92:d1:bb:fc:b7:fa:58:7f:4d:2b:c5:59 77:78:8e:14:b7:a7:d4:28:c6:50:3a:cf:1a:2d:35:35 51:df:10:bc:d7:16:95:b8:de:5f:1f:36:d2:71:8a:8a  Examining the Signature Since the signature is vital to the integrity of the certificate's contents, we really should look into it more. George Bolo wrote specifically about how to do this. What we need to do is just tell openssl to not print a lot of the default fields when using -text. You can find George's blog entry here \u0026lt;https://linuxctl.com/2017/02/x509-certificate-manual-signature-verification/\u0026gt;.\nopenssl x509 -in cert.pem -text -noout -certopt ca_default -certopt no_validity -certopt no_serial -certopt no_subject -certopt no_extensions -certopt no_signame  Next we'll need to take that output, which is the signature of the CA, with RSA encryption. The key that was used to encrypt this hash was the issuer's public key. So we need to take that\nopenssl rsautl -verify -inkey \\ \u0026lt;(openssl x509 -in certs/RootCA_GnuTLS-RSA.crt -noout -pubkey) -in \\ \u0026lt;(openssl x509 -in certs/IntermediateCA_GnuTLS-RSA.crt -text -noout -certopt ca_default -certopt no_validity -certopt no_serial -certopt no_subject -certopt no_extensions -certopt no_signame | grep -v 'Signature Algorithm' | tr -d '[:space:]:' | xxd -r -p) -pubin \\ | openssl asn1parse -inform der     RootCA ECC generate the key, then generate the request, otherwise known as the Certificate signing request. The req request takes the request and immediately signs it and generates the certificate. Root certicates are always self signed, these are the highest level of the trust tree. Feel free to give this certificate out. If you own this machine, distribute it, and trust it (especially in a lab/sandbox).\nopenssl ecparam -genkey -name prime256v1 -out ${KEY_PATH}/RootCA_ECC.key openssl req -new -x509 -sha256 -days 2191 -key ${KEY_PATH}/RootCA_ECC.key -reqexts v3_req -extensions v3_ca -out ${CERT_PATH}/RootCA_ECC.crt  Note\nOptional, we can do these operations as different commands, if you are learning this might be a good route, just to see what the command above is doing.\n openssl req -new -sha256 -nodes -key ${KEY_PATH}/RootCA_ECC.key -reqexts v3_req -out ${CSR_PATH}/RootCA_ECC.csr \u0026amp;\u0026amp; \\ openssl x509 -req -sha256 -signkey ${KEY_PATH}/RootCA_ECC.key -in ${CSR_PATH}/RootCA_ECC.csr -extensions v3_ca -out ${CERT_PATH}/RootCA_ECC.crt  Optional (generates the revoke list) Here a certificate revoke list is being generated. A certificate revoke list a location where clients can check to see if the issuer has revoked the certificate they are about to consume. These are typically not used day-to-day, and OCSP (stapling) is favored. Since OCSP prevents SSL Stripping, which can be done by proxies. If you want something funny, look at the bug of man openssl-crl\nopenssl crl -inform PEM -in ${KEY_PATH}/RootCA_ECC.key -CAfile ${CERT_PATH}/RootCA_ECC.crt -outform DER -out ${REVOKE_PATH}/RootCA_ECC.crl    IntermediateCA ECC These step could be repeated for a client certificate authority; it might be good so that this CA handles only users and will be need to be explicitly added to which ever trust will be handling the users. Additionally this could be given to an intern or contractor, and if the CA key/secret is compromised, there is limited impact to other CA's.\nopenssl ecparam -genkey -name prime256v1 -out ${KEY_PATH}/IntermediateCA_ECC.key openssl req -new -sha256 -nodes -key ${KEY_PATH}/IntermediateCA_ECC.key -out ${CSR_PATH}/IntermediateCA_ECC.csr #CSR's openssl x509 -req -days 1200 -sha256 -in ${CSR_PATH}/IntermediateCA_ECC.csr -CAkey ${KEY_PATH}/RootCA_ECC.key -CA ${CERT_PATH}/RootCA_ECC.crt -out ${CERT_PATH}/IntermediateCA_ECC.crt -CAcreateserial -CAserial ${SRL_PATH}/IntermediateCA_ECC.srl #optional: -set_serial 01  CA Answers If you want to make an \u0026quot;answers\u0026quot; file, this will allow you to by pass many of OpenSSL's prompts. When dealing with a lot of certificates this is very useful. Here is one for a Certificate Authority. Since by now you've already worked through a prompt with the Root CA, you should pretty much understand what the fields are used for.\n[req] prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C=US ST=North Carolina O=LazyTree localityName= commonName=*.lazytree.us organizationalUnitName=HomeLab emailAddress=kondor6c\u0026#64;lazytree.us  Here we'll generate a server certificate with the same encryption type. But we'll do something a little special here. We'll specify extensions to the X509 certificate types. These are added on top of the X509 certificates, the really improve things good deal and chances are you'll need them, almost 100% you'll need Subject Alternate Names, typically just called \u0026quot;SANs\u0026quot;. The following is pretty much copy pastable, if you are in a pinch grab and go, replace some of the unique fields like file names and Locality type repsonses.\nopenssl ecparam -genkey -name prime256v1 -out wildcard_lazytree_ECC.pem openssl req -new -sha256 -key wildcard_lazytree_ECC.pem -out ${CSR_PATH}/wildcard_lazytree_ECC.csr -config \u0026lt;( cat \u0026lt;\u0026lt;-EOD [req] prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C=US ST=North Carolina O=LazyTree localityName=Redacted OU=HomeLab emailAddress=kondor6c\u0026#64;lazytree.us CN=null.lazytree.us [ req_ext ] subjectAltName = \u0026#64;alt_names [ alt_names ] DNS.1 = expired.lazytree.us DNS.2 = testing.lazytree.us DNS.3 = lazytree.us EOD openssl x509 -req -days 800 -sha256 -in ${CSR_PATH}/IntermediateCA_ECC.csr -CAkey IntermediateCA_ECC.pem -CAserial RootCA_ECC.srl -out ${CERT_PATH}/wildcard_lazytree_ECC.crt   Serial The first time you use your CA to sign a certificate you can use the -CAcreateserial option. This option will create a file (ca.srl) containing a serial number. You are probably going to create more certificate, and the next time you will have to do that use the -CAserial option (and no more -CAcreateserial) followed with the name of the file containing your serial number. This file will be incremented each time you sign a new certificate. This serial number will be readable using a browser (once the certificate is imported to a pkcs12 format). And we can have an idea of the number of certificate created by a CA.\n  Clients trust This will allow clients to use certificate in a two manner there are many exampes of big projects that have support of this (but not limited to): postgres dovecot mysql HAProxy Apache nginx curl kafka    I hope the list jogs your mind on where you can take this, two way SSL or \u0026quot;mTLS\u0026quot; or Mutual Authentication is really just allowing the client (the one connecting to the server) to specify a certificate, this is done at the client portion of the TLS handshake, which we'll dig into soon. Let's go ahead and generate the client cert here. I mentioned at the beginning of this documentation that I would try to use a different Intermediate for usage as a client CA. This is because you'll typically need to distribute this CA to clients, and might need to give access to the intermediates to other teams, like a client satisfaction team or sales engineers to issue new client certs quickly. This is just an example, not a best practice.\nopenssl ecparam -genkey -name prime256v1 -out ${client_key_out} openssl req -new -sha256 -key ${client_key_out} -out ${CSR_PATH}/client_lazytree_ECC.csr openssl x509 -req -days 300 -sha256 -in ${CSR_PATH}/client_lazytree_ECC.csr -CA ${CERT_PATH}/ IntermediateClientCA_ECC.crt -CAkey IntermediateClientCA_ECC.pem -out ${CERT_PATH}/client_lazytree_ECC.crt  Chains or Bundles Chains can be used, or they don't have to be. The usage lies in the fact that if an intermediate is not trusted, but the root certificate is, or another intermediate in the chain is trusted. The name bundles are used because there are bundles of certificates (Root and Intermediates), it is highly recommended that the fully chain, be sent (hey you reading this, send dat chain!). You can find options that are used for CA Chains in the server secition below. The order is defined in RFC-5246 \u0026lt;https://tools.ietf.org/html/rfc5246#section-7.4.2\u0026gt; The order is exactly as follows: Server Certificate Intermediate \u0026lt;optional\u0026gt; another Intermediate that has signed number two Root Certificate    cat IntermediateCA.crt RootCA.crt \u0026gt; Cert-Chain.pem cat IntermediateCA_ECC.crt RootCA_ECC.crt \u0026gt; Cert-Chain_ECC.pem    Verification of Certificates It is always good to verify your work, even better to have a buddy check your work too, you never know what you might learn from somebody else's perspective.\nExamine a certificate Check your work\nopenssl x509 -in ${CERT_PATH}/certificate.crt -text -noout  Examine a key (RSA) You can also look at the key you produced\nopenssl rsa -in privateKey.key -check   Public key (RSA) Sometimes it can be useful to have the public key of the secret private key.\nopenssl rsa -in privateKey.pem -pubout -out publicKey.pem   Examine a Certificate Signing Request (CSR) To view a previously generated certificate signing request you can run the following.\nopenssl req -text -noout -verify -in CSR.csr    Revoke a Cert As mentioned, revokation lists and the revoking process isn't done too much. But it could really help out, consider an example, 24 hours before a certificate is about to expire if an Internal CA were to revoke the soon to expire certificate, you will have an opportunity to know for sure which applciations depend on the certifcate. This could be very useful for large organizations. Just a tip!\nopenssl ca -config ca.conf -revoke ia.crt -keyfile ca.key -cert ${CERT_PATH}/ca.crt -crl_reason superseded     Configuring SSL on Operating Systems Here is a list of operating systems and how to configure SSL on them, I hope this helps, if you know of somelet me know (open a pull request).\nWindows First we need to prep, the the best of my knowledge windows doesn't handle pem formats, which is pretty frustrating. So we need to export it to a PKCS12 format.\nopenssl pkcs12 -export -in wildcard_lazytree_ECC.crt -inkey wildcard_lazytree_ECC.pem -out wildcard_lazytree_ECC.pfx -certfile Cert-Chain_ECC.pem openssl pkcs12 -export -in wildcard_lazytree.crt -inkey wildcard_lazytree.pem -out wildcard_lazytree.pfx -certfile Cert-Chain.pem openssl pkcs12 -export -nokeys -in RootCA.crt -out RootCA.pfx openssl pkcs12 -export -nokeys -in RootCA_ECC.crt -out RootCA_ECC.pfx  Now we can take that file and add it to Windows\ncertutil.exe -addstore \u0026quot;RootCA_SHA1\u0026quot; RootCA.pfx certutil.exe -addstore \u0026quot;RootCA_ECC\u0026quot; RootCA_ECC.pfx certutil.exe -importPFX wildcard_lazytree_ECC.pfx certutil.exe -importPFX wildcard_lazytree.pfx   RHEL-like Linux You can easily add certificates to Redhat like distributions like Fedora, Centos, Amazon Linux, Scientific Linux or Oracle Linux. Consider distributing this as an RPM.\nrsync -va \\*crt /etc/pki/ca-trust/source/anchors/ update-ca-trust force-enable   Debian-like Linux AND Gentoo Gentoo \u0026lt;https://wiki.gentoo.org/wiki/Certificates\u0026gt; https://www.archlinux.org/news/ca-certificates-update/\nrsync -va \\*crt /usr/local/share/ca-certificates/ update-ca-certificates   Android Settings \u0026gt; Security \u0026amp; Lock Screen \u0026gt; Credential storage (under \u0026quot;advanced\u0026quot;) \u0026gt; Install from storage\n  Applications Java Java holds the keys and certificates in a special file, called a keystore. It used to be a proprietary format JKS, but the newer, preferred format is p12 (PKCS12). You can access it with keytool, which should be in the same path as java ($JAVA_HOME/bin/).\nkeytool -v -list -keystore /etc/pki/ca-trust/extracted/java/cacert || keytool -v -list -keystore /etc/pki/java/cacerts #changeit is Java's default keytool -import -trustcacerts -alias rootCA_ECC -file RootCA_ECC.crt keytool -import -trustcacerts -alias IntermediateCA_ECC -file IntermediateCA_ECC.crt keytool -import -trustcacerts -alias rootCA_weak -file RootCA.crt keytool -import -trustcacerts -alias IntermediateCA_weak -file IntermediateCA.crt   Chrome https://stackoverflow.com/questions/7580508/getting-chrome-to-accept-self-signed-localhost-certificate You can avoid the message for trusted sites by installing the certificate. This can be done by clicking on the warning icon in the address bar, then click \u0026quot;Not secure\u0026quot; -\u0026gt; Certificate Invalid -\u0026gt; Details Tab -\u0026gt; Export... Save the certificate.\nUse Chrome's Preferences -\u0026gt; Under The Hood -\u0026gt; Manage Certificates -\u0026gt; Import. On the \u0026quot;Certificate Store\u0026quot; screen of the import, choose \u0026quot;Place all certificates in the following store\u0026quot; and browse for \u0026quot;Trusted Root Certification Authorities.\u0026quot; Restart Chrome. Chrome Settings \u0026gt; Show advanced settings \u0026gt; HTTPS/SSL \u0026gt; Manage Certificates \u0026gt; Authorities\n Nginx https://www.digitalocean.com/community/tutorials/how-to-create-a-self-signed-ssl-certificate-for-nginx-on-centos-7\nserver { listen 80; server_name \u0026quot;example.lazytree.us\u0026quot;; return 301 https://$host$request_uri;  } server { server_name \u0026quot;10.1.1.1\u0026quot; listen 443 http2 ssl; listen [::]:443 http2 ssl; ssl_certificate /etc/ssl/certificates/example.lazytree.us/app_role.crt; ssl_certificate_key /etc/ssl/keys/example.lazytree.us/app_role.key; ssl_dhparam /etc/ssl/keys/example.lazytree.us/dhparam.pem;  }   Apache https://wiki.apache.org/httpd/RedirectSSL\nListen 443 ssl \u0026lt;VirtualHost _default_:443\u0026gt; ServerName lazytree.us SSLEngine on SSLProtocol all -SSLv2 -SSLv3 SSLCertificateFile /etc/pki/tls/certs/ SSLCertificateKeyFile /etc/pki/tls/private/ SSLCertificateChainFile /etc/pki/tls/certs/chain.crt SSLCACertificateFile /etc/httpd/conf.d/tls/client_IntermediateCA.crt SSLOpenSSLConfCmd DHParameters \u0026quot;/etc/pki/ssl/dhparams.pem\u0026quot; RewriteEngine On RewriteCond %{HTTPS} off RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L]  \u0026lt;/VirtualHost\u0026gt;  # It would be nice to get blake2s256 supported in more places #GPG fingerprint = 7545BFF3710684D2E6BCFE98C5D5F4BED24A4A02 #GPG fingerprint = 438263E03BF0BDC64F9A6415AA63E0576CC60292\n  GNU TLS I have recently been liking GnuTLS since it has rather descriptive options, they are easy to read and self describing of the process. The issue is that it isn't always installed.\ncerttool --generate-privkey --bits 4096 --outfile RootCA_G-RSA.pem certtool --generate-request --load-privkey RootCA_G-RSA.pem --hash=SHA256 --template gnutls-ssl-answers.txt --outfile RootCA_G-RSA.csr certtool --generate-certificate --load-privkey RootCA_G-ECC.pem --outfile RootCA_G-ECC.crt --load-ca-certificate ca-cert.pem --load-ca-privkey ca-key.pem  GNU TLS ECC Coming in version 3.6!! Ed25519 keys certtool --generate-privkey --key-type ed25519 --outfile RootCA_G-ECC.pem otherwise you might need to go with secp 256\ncerttool --generate-privkey --ecc --curve secp256r1 --outfile RootCA_G-ECC.pem certtool --generate-request --load-privkey RootCA_G-ECC.pem --hash=SHA256 --outfile RootCA_G-ECC.csr certtool --generate-certificate --load-privkey RootCA_G-ECC.pem --outfile RootCA_G-ECC.crt --load-ca-certificate ca-cert.pem --load-ca-privkey ca-key.pem    Quick Reference Lists You could call these \u0026quot;cheat sheets\u0026quot; but these are more translation matrixes, like a rosetta stone of options. I often get frustrated with how many different options I'm called to remember (not entirely told, but just feel as though, professionally, I should). It can be difficult not specializing in a specific peice of software, since you have a ever expanding target of defaults, types of actions, configurations locations, and command line arguements; but I digress. I hope to make more of these, checkout my dotfiles where I use the top 26 lines as a quick reference. Some the options I know, others I have a hard time remembering, while others I learned while making it. https://github.com/kondor6c/dotfiles\n verification   OpenSSL GnuTLS function   x509 -verify --verify verify x509 cert  x509 -CAfile --load-ca-certificate verify chain CA file  x509 -text -noout -in --certificate-info --infile verify an x509 cert  req -noout -text -in --crq-info --infile examine a CSR     x509 generation   OpenSSL GnuTLS function   x509 -text -noout -in --verify --infile verify x509 certicate  x509 -CAfile --load-ca-certificate verify chain againt file  x509 -CAkey --load-ca-privkey load CA key to sign  x509 -req --load-request load CSR to sign for cert  x509 -CA --load-ca-certificate Load CA cert to sign  x509 -config --template preconfigured answers  x509 -sha256 --hash=SHA256 certificate hash (sha256)     General   OpenSSL GnuTLS function   s_client -connect :443 gnutls-cli HOST connect to $HOST:443  x509 -CAfile --load-ca-certificate verify chain againt file     keys (RSA)   OpenSSL GnuTLS function   genrsa -out --generate-privkey --outfile Write rsa Key to file  rsa -noout -text -in -k --infile examine RSA Key     Diffie Helman   OpenSSL GnuTLS function   dhparam 2048 -out --generate-dh-params generate parameters     ref links and stuff https://jamielinux.com/docs/openssl-certificate-authority/create-the-root-pair.html\n RFC list Forward Secrecyree_SSH_CA-ed -h -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH_CA-ecdsa -h -t ecdsa -b 521 ssh-keygen -f LazyTree_SSH_CA-rsa -h -t rsa -b 4096\nTraditional Host Keys ssh-keygen -f LazyTree_SSH-rsa -t rsa -b 4096 ssh-keygen -f LazyTree_SSH-ed -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH-ecdsa -t ecdsa -b 521\n DH Params Diffie Helman is pretty cool\nopenssl dhparam -out dhparam.pem 4096    OpenSSH CA https://blog.habets.se/2011/07/OpenSSH-certificates.html\n Host CA's ssh-keygen -f LazyTree_SSH_CA-ed -h -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH_CA-ecdsa -h -t ecdsa -b 521 ssh-keygen -f LazyTree_SSH_CA-rsa -h -t rsa -b 4096\nTraditional Host Keys\nssh-keygen -f LazyTree_SSH-rsa -t rsa -b 4096 ssh-keygen -f LazyTree_SSH-ed -t ed25519 -a 100 ssh-keygen -f LazyTree_SSH-ecdsa -t ecdsa -b 521\n "
},
{
	"uri": "https://kondor6c.github.io/posts/btrfs_scrub_fail/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " title: btrfs Scrub Fail date: 2016-05-05 category: devops tags: - linux - btrfs slug: btrfs_srub_fail\ndescription: I experience unrecoverable error on btrfs RAID 5, ignoring previous claims of instability btrfs scrub fail {{tag\u0026gt;btrfs}} I wrote this earlier on a internal corporate blog (Atlassian\u0026rsquo;s Confluence), but I don\u0026rsquo;t think many read it, it doesn\u0026rsquo;t give out any corporate information, but since I wrote it on \u0026ldquo;company time\u0026rdquo; I figured I should put it there. But with the late night phone calls and the subsequent difficult time in falling asleep I think a few minutes here and there all balances out. That\u0026rsquo;s another topic for another time. Here is a repost of the blog. I\u0026rsquo;m really only putting it here for the sake of context and centralizing my posts. This was written on May 11th 2016.\nbtrfs I have been trying/using btrfs. I have really wanted to give it a chance even and I thought a perfect place for that would be at home. So a little bit of background my home machine is a Xeon E5-2620 v3 and it\u0026rsquo;s got 32GB of ECC RAM, I have 4x3TB disks that are in a btrfs RAID 5 setup, it mostly holds torrents (all legal of course), virtual machine disks, and backups (old army documents, pictures). Things had been going rough when I first tried using btrfs (on my root volume), but that turned out to be a bad SSD, nothing to do with the RAID 5. So I kept ext4 on my SSD (rootfs) and I continue to use btrfs on the RAID. Just last week I had a disk fail due to bad sectors, I had time to replace it but after the replace operation.\n[root@horse ~]# btrfs replace start /dev/sdf /dev/sde /mnt/extra I continue to get unrepairable errors\u0026hellip;. btrfsck doesn\u0026rsquo;t address it, btrfs scrub on finds the errors. The volume mounts, and unmounts, it just seems that I should be able to address these without going into a recovery, I think it happened during the course of the replace operation. I was able to move the files to a different volume, but it doesn\u0026rsquo;t seem to make a difference. I found the files by looking at the inode that the checksum failed with. As I continue to address these errors I\u0026rsquo;ll keep you posted on the status of my btrfs experience. I was hesitating using ZFS due to the licensing issues. I have long been a fan of ZFS, I think I might try it now; after all I have ECC RAM which is the recommendation. Another thought is to just delete the disk, then re-add it, since it seems to be just one that has the errors.\n[root@horse ~]# btrfs scrub status /mnt/extra scrub status for 8289c25f-a8d1-44aa-8c18-6215831d2cae scrub started at Thu May 5 12:17:58 2016 and finished after 40:13:09 total bytes scrubbed: 4.52TiB with 12 errors error details: read=4 csum=8 corrected errors: 0, uncorrectable errors: 12, unverified errors: 0 [root@horse ~]# btrfsck --repair /dev/sda enabling repair mode Checking filesystem on /dev/sda UUID: 8289c25f-a8d1-44aa-8c18-6215831d2cae checking extents checking free space cache checking fs roots checking root refs found 4961408222736 bytes used err is 0 total csum bytes: 4837631228 total tree bytes: 6034817024 total fs tree bytes: 754761728 total extent tree bytes: 84541440 btree space waste bytes: 420880645 file data blocks allocated: 42712031051776 referenced 4948738207744 [root@horse ~]# mount /mnt/extra/ [root@horse ~]# dmesg |grep btrfs [703225.538312] btrfs[10452]: segfault at 30 ip 00005647596ed880 sp 00007fff57c1bb40 error 4 in btrfs[564759698000+92000] [703232.764835] btrfs[10521]: segfault at 30 ip 00005597082f2880 sp 00007fff2371b050 error 4 in btrfs[55970829d000+92000] [root@horse ~]# #echo \u0026#34;segfaults with ECC?\u0026#34; [root@horse ~]# dmesg |tail -n3 [843497.711057] BTRFS info (device sda): relocating block group 12359603060736 flags 129 [843517.301202] BTRFS info (device sda): found 366 extents [843519.453459] BTRFS info (device sda): found 366 extents [843519.913111] BTRFS info (device sda): relocating block group 12356381835264 flags 129 [843539.377181] BTRFS info (device sda): found 366 extents [843540.827521] BTRFS info (device sda): found 366 extents [843720.099900] BTRFS info (device sda): disk space caching is enabled [843720.099907] BTRFS: has skinny extents [843720.995888] BTRFS info (device sda): bdev /dev/sde errs: wr 0, rd 76, flush 0, corrupt 8, gen 0 [843721.573752] BTRFS: checking UUID tree [root@horse ~]# rpm -qa |grep btrfs-pr btrfs-progs-4.4.1-1.fc23.x86_64 [root@horse ~]# uname -a Linux horse 4.4.8-300.fc23.x86_64 #1 SMP Wed Apr 20 16:59:27 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux"
},
{
	"uri": "https://kondor6c.github.io/tags/blogging/",
	"title": "Blogging",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/btrfs/",
	"title": "Btrfs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/certs/",
	"title": "Certs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/community/",
	"title": "Community",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/conferences/",
	"title": "Conferences",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/emacs/",
	"title": "Emacs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/fedora/",
	"title": "Fedora",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/gnutls/",
	"title": "Gnutls",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/golang/",
	"title": "Golang",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/hardware/",
	"title": "Hardware",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/learning/",
	"title": "Learning",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/linux/",
	"title": "Linux",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/non-tech/",
	"title": "Non Tech",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/note_taking/",
	"title": "Note_taking",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/openssl/",
	"title": "Openssl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/posts/",
	"title": "Posts",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/qa/",
	"title": "Qa",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/real-world/",
	"title": "Real World",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/remote/",
	"title": "Remote",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/video/",
	"title": "Video",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/tags/zfs/",
	"title": "Zfs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/",
	"title": "blog",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://kondor6c.github.io/posts/pyenv/",
	"title": "pyenv",
	"tags": ["linux"],
	"description": "Version managers for languages",
	"content": " Version Managers I have been enjoying the idea of version managers for interpreted languages like Ruby, Python and even NodeJS. Python likes to advocate using virtualenv, which works very well when interacting and writing the code. However when interacting with automation like Ansible or Chef this becomes difficult since you have to source the shell scripts to install a given version.\nSome might argue that there isn\u0026rsquo;t a need to have one of these version managers, and as you might guess I disagree with that, and advocate for them. If you lock in the version that you are testing/developing with this allows for more stability (no surprises is a good thing in production). You can then bring in a new version by putting it through a testing suite. This way you are not constrained to what version that the operating system is doing, it would be nice if the code worked correctly despite a new version, but frequently these are factors out of our control. When your building your project, setup a version manager might require a little bit of work but what if you get a little distracted (personal project focused) or get pulled my from a given project (corporate focused), having the version locked removes a lot of potential pain. Additionally this would allow moving to docker since all you need to do is setup that version manager and you could be on Arch Linux in a docker container.\nWe don\u0026rsquo;t know what the future holds, if we make our projects explicit on the versions it needs (even dependencies), then we are increasing the stability.\nI frequently try to argue the other side, and two arguments I can see is that it\u0026rsquo;s easier to just do \u0026ldquo;yum install ruby-devel\u0026rdquo; or that it increases the likelihood that package requirements/dependencies become stale. The ease of use of many version managers are increasingly more simple, all you need to do is switch to the development user and pipe the install script from curl to bash. Now the former I might have to agree with you on, and generally you could specify in the requirements that a \u0026ldquo;greater than\u0026rdquo; version is acceptable and lock it when you feel it is a concern. To add credibility to my statements I have used [[https://rvm.io/|RVM]] in production in these examples gitlab, a custom application, and with Canvas LMS. I have code in place for [https://github.com/creationix/nvm|NVM] and I am investigating [[https://github.com/yyuu/pyenv-installer|pyenv]] for Python.\n"
}]